\documentclass[11pt,a4paper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{hyperref}
\usepackage{natbib}

\title{Neural dialogue act tagging with transformer pre-training (or something like that...)}

\author{
  First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
Affiliation / Address line 3 \\
  {\tt email@domain} \\\and
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\
}
 

\date{}

\begin{document}
\maketitle

\begin{abstract}
  abstract
\end{abstract}

\section{Introduction}
\begin{itemize}
  \item \cite{austinHowThingsWords2009} -- Speech acts
  \item \cite{coreCodingDialogsDAMSL1997} -- DAMSL dialogue act scheme
  \item \cite{devlinBERTPretrainingDeep2018} -- BERT
    Main questions:
    1. Are pre-trained transformer encoding (BERT) helpful for representation in a dialogue task (speficially, dialogue act tagging)?
    2. Are additional dialogue specific features helpful (discourse markers, disfuency, laughter)?
\end{itemize}

\section{Related Work}
\begin{itemize}
  \item \cite{cerisaraEffectsUsingWord2vec2017}
  \item \cite{kalchbrennerRecurrentConvolutionalNeural2013}
  \item \cite{pragstVectorRepresentationUtterances2018}
  \item \cite{sordoniNeuralNetworkApproach2015}
  \item \cite{stolckeDialogueActModeling2000} - HMM dialogue act tagging
  \item \cite{khanpourDialogueActClassification}
  \item \cite{tranPreservingDistributionalInformation2017}
\end{itemize}

\section{Model}
The proposed model has two components: an utterance encoder, and a sequence model that predicts the dialogue act tag.
For the sequence model, we use a simple RNN.
Conceptually the hidden state of this RNN represents the discourse context which, 
together with the encoded utterance, is used to predict the discourse act tag.

\section{Experiments}
Our aim is to test the effectiveness of different utterance encoding schemas. 
\begin{enumerate}
  \item Average word embedding (word2vec)
  \item CNN (with/without word2vec initalization)
  \item BERT encoded sentences (with/without additional unsupervised pretraining)
  \item Average BERT word embeddings
\end{enumerate}

\bibliography{abstract}{}
\bibliographystyle{acl_natbib}

\end{document}
