%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for ACL 2020 Proceedings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Basically we want to see if language models pre-trained on text data are useful for dialogue.
  So we look at if BERT is useful for dialouge act recoginiton.
  But we want to know if it's really adapting to the domain, so we look at how it uses laughter, a phenomenon specific to dialogue.
\end{abstract}

% INTRODUCTION

Recently, large-scale language models, trained on massive corpora of text data, have achieved state-of-the-art results on a variety of traditonal NLP tasks.
We investigate whether such models might be useful for processing dialogue, and if so, whether they adapt to make use of dialogue-specific features.

Dialogue, especially spoken dialogue, is radically different from the kind of data that neural language models are prestrained on.
Not only is the internal structure of contributions different---with features such as disfluencies, repair, incomplete sentences, and various vocal sounds---but the sequential structure of the discourse is also very different.
In dialouge speakers take turns and switch perpsectives.
More generally, the way that utterances contribute and cohere to the discourse is different from the relationship between sentences in a piece of written text.

One important dialogical feature missing from text data is laughter.
In the Switchboard dialogue corpus, laughter appears in approximately \%X of utterances.
Laughter also relates to the discourse structure of dialogue. 
Laughables.
Also useful for predicting DAs (is that already a result?)

In this work, we investigate whether BERT, a well-know pre-trained language model, is useful for dialogue.
We use dialogue act recoginition as a proxy task, since both the interal content of conributions, and their sequential structure have bearing on the task.

\section{Background}
%  - dialogue acts
%  - laughter
%  - BERT 

\section{Related work}
%  - BERT domain transfer
%  - BERT/LM for DAR

\section{Data}
%  - AMI, SWBD + comparison table
%  - Laughter with/without
  
\section{Model}
%  - DAR model -> utt encoder, RNN sequence tagger
%  - BERT encoder (12 heads, uncased,...)
%  - baseline encoder

\section{Experiments}
%  - BERT vs. baseline (L/NL)
%  - in-domain pre-training (L/NL)
%  - cross-domain pre-training

\section{Results and discussion}

\bibliography{anthology,acl2020}
\bibliographystyle{acl_natbib}

\end{document}
