%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{natbib}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{tabularx}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Pre-trained encoders for dialogue: The case of laughter}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  The degree to which language models pre-trained on text data are useful for dialogue is an open question.
  In this paper, we investigate the usefulness of BERT for dialogue act recognition.
  We are interested in both how useful the pre-training proceedure is and 
  how well the model adapts to the dialogue domain.
  To confirm that the fine-tuned model learns to represent dialogical features, 
  we look at how it uses laughter, a phenomenon specific to dialogue.
\end{abstract}

% INTRODUCTION

Recently, large-scale language models, trained on massive corpora of text data, have achieved state-of-the-art results on a variety of traditional NLP tasks.
It is still uncertain whether these models might be useful for processing dialogue, and if so, whether they can adapt to make use of dialogue-specific features.

Dialogue, especially spoken dialogue, is radically different from the kind of data that neural language models are pre-trained on.
In the example from the Switchboard corpus, shown in table~\ref{table:example}, it is evident that the syntactic structure of dialogue is very different from  written text, even if one imagines a script for a movie or a play.
Not only is the internal structure of contributions different---with features such as disfluencies, repair, incomplete sentences, and various vocal sounds---but the sequential structure of the discourse is also very different.
In dialogue, speakers take turns and switch perspectives.
More generally, the way that utterances contribute and cohere to the discourse is different from the relationship between sentences in a piece of written text.

One important dialogical feature missing from text data is laughter.
In the Switchboard dialogue corpus, laughter comes about every 200 tokens.
Laughter also relates to the discourse structure of dialogue. 
% Laughables.
% Also (known to be?) useful for predicting DAs 

\begin{table}
  \centering
  \begin{tabularx}{\linewidth}{llL}
    \toprule
    Speaker & DA & Utterance \\ \midrule
    A	&sd	& Well, I'm the kind of cook that I don't normally measure things,  \\
    A	&sd	& I just kind of throw them in \\
    A	&sd	& and, you know, I don't to the point of, you know, measuring down to the exact amount that they say.  \\
    B	&sv	& That means you're a real cook. \\
    A	&bd	& \texttt{<Laughter>} Oh, is that what it means.  \\
    A	&b	& Uh-huh.  \\
    A	&x	& \texttt{<Laughter>}.\\
             \bottomrule
  \end{tabularx}
  \caption{Example from the SWDA corpus (sw2827). Dialogue acts: \emph{sd}---Statement-non-opinion, \emph{sv}---Statement-opinion, \emph{bd}---Downplayer, \emph{b}---Backchannel, \emph{x}---Non-verbal. }
  \label{table:example}
\end{table}

In this work, we investigate whether BERT \citep{devlinBERTPretrainingDeep2018}, a well-know pre-trained language model, is useful for dialogue.
We use dialogue act recognition  as a proxy task, since both the internal content of contributions, and their sequential structure have bearing on the task.

\paragraph{Dialogue Act Recognition (DAR)}
The concept of a dialogue act is based on that of speech acts \citep{austinHowThingsWords2009}.
Breaking with classical semantic theory, speech act theory considers not only the propositional content of an utterance but also the actions, such as \emph{promising} or \emph{apologizing}, it carries out.
Dialogue acts extend the concept of the speech act, with a focus on the interactional nature of most speech.
DAMSL \citep{coreCodingDialogsDAMSL1997}, for example, is an influential dialogue act tagging scheme where dialogue acts are defined in part by whether they have a \emph{forward-looking} function (are expecting a response) or \emph{backward-looking} function (are in response to a previous utterance).

DAR is the the task of labeling utterances with the dialogue act they perform 
from a given set of dialogue act tags.
As with other sequence labeling tasks in NLP, some notion of context is helpful in DAR.
One of the first performant machine learning models for DAR was a Hidden Markov Model that used various lexical and prosodic features as input \citep{stolckeDialogueActModeling2000}.
Most successful neural approaches also model some notion of context, 
for example with a recurrent model \citep[e.g.,][]{botheContextbasedApproachDialogue2018} or conditional random field \citep[e.g.,][]{chenDialogueActRecognition2017}.

\paragraph{Transfer learning for NLP}
\begin{itemize}
  \item \citet{mikolovDistributedRepresentationsWords2013} -- word2vec
  % \item \citet{devlinBERTPretrainingDeep2018} -- Original BERT paper 
  \item \citet{sunHowFineTuneBERT2019} -- recommendations for fine-tuning BERT
  \item \citet{petersTuneNotTune2019} -- when and how to fine-tune pre-trained representations
\end{itemize}


\paragraph{Contributions}
The main results of this paper have to do with to the usefulness of language model fine-tuning for DAR, the impact of laughter on DAR, and the relationship between the two.
\begin{itemize}
  \item Laughter is useful for dialogue act recognition, and its impact varies across different dialogue acts (\S\ref{sec:experiment1}).
  \item Standard BERT pre-training is useful for DAR, but the impact of pre-training is smaller than than of fine-tuning (\S\ref{sec:experiment2})
  \item Performing additional in-domain pre-training shows promise for dialogue (\S\ref{sec:experiment3}), but further investigation with larger dialogue corpora is required.
\end{itemize}


\section{Data}
We perform experiments on the Switchboard Dialogue Act Corpus (SWDA), which is a subset of the larger Switchboard corpus, and the dialogue act-tagged portion of the AMI Meeting Corpus (AMI-DA). 
SWDA is tagged with a set of 220 dialogue act tags which, following  \citet{jurafskySwitchboardSWBDDAMSLShallowDiscourseFunction1997a} we cluster into a smaller set of 42 tags.
AMI uses a smaller tagset of 16 dialogue acts \citep{GuidelinesDialogueAct2005}.

% - DA distribution              % Vlad

\begin{table}[]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Switchboard}       & \textbf{AMI Corpus}                     \\ \midrule
Dyadic                     & Multi-party                             \\
Casual conversation        & Mock business meeting                   \\
Telephone                  & In-person \& video                      \\ \midrule
English                    & English                                 \\ 
Native speakers            & Native \& non-native speakers           \\ 
early '90s                 & 2000s                                   \\ \midrule
2200 conversations         & 171 meetings                            \\
  \hspace{1em} 1155 in SWDA               & \hspace{1em} 139 in AMI-DA                           \\
400k utterances             & 118k utterances                         \\
3M tokens                  & 1.2M tokens                             \\ \bottomrule
\end{tabular}
  \caption{Comparison between Switchboard and the AMI Meeting Corpus}
  \label{table:corpora}
\end{table}

The distribution of laughs in different dialogue acts has a rather uniform shape with a few outliers (see box plots). Figures \ref{fig:swda-by-da} (for SWDA) and \ref{fig:ami-by-da} (for AMI-DA) provide a breakdown of dialogue act depending on whether current or adjacent utterances (the preceding utterance and the following one are counted as adjacent) contain a laughter. Note that SWDA has a ``Nonverbal'' dialogue act which is misguiding with respect to laughter, because utterances only containing a single laughter token fall into this category, however they can serve, for example, to acknowledge a statement or to deflect a question  \citep{mazzocconi2019phd}.

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{img/box-swda.pdf}
  \caption{Box plot for utterances associated with laughter in SWDA. On the left: proportion of DAs having laughter in one of the adjacent utterances, on the right: proportion of DAs containing laughter. }
    \label{fig:box-swda}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{img/box-ami.pdf}
  \caption{Box plot for utterances associated with laughter in AMI-DA. On the left: proportion of DAs having laughter in one of the adjacent utterances, on the right: proportion of DAs containing laughter. }
    \label{fig:box-ami}
\end{figure}

For SWDA corpus we also considered grouping the dialogue acts according to DAMSL coders manual \citep{jurafskySwitchboardSWBDDAMSLShallowDiscourseFunction1997a}: Forward-Communicative-Function (51.86\% of all DAs), Backwards-Communicative-Function (30.35\%), Other (9.04\%) and Communicative-Status (8.75\%). We observed even distribution of laughs in these groups, except the Communicative-Status group which contains ``Nonverbal'' dialogue acts.

% - Preprocessing: remove disfluencies, acronyms and speaker tokens in AMI, removing laughter % Bill
  
\section{Model} % Bill

To test the effectiveness of BERT for DAR, we employ a simple neural architecture with two components: an encoder that vectorizes utterances, and a sequence model that predicts dialogue act tags from the vectorized utterances (figure \ref{fig:model-architecture}).
Since we are primarily interested in comparing different utterance encoders, we use a basic RNN as the sequence model in every configuration. 
The RNN takes the encoded utterance as input at each time step,
and its hidden state is passed to a simple linear classification layer over dialogue act tags.
Conceptually, the encoded utterance represents the context-agnostic semantic content of the utterance, and the hidden state of the RNN represents the full discourse context.

\begin{figure*}
  \include{model}
  \caption{Simple neural dialogue act recognition sequence model}
  \label{fig:model-architecture}
\end{figure*}

As a baseline utterance encoder, we use a word-level CNN with window sizes of 3, 4, and 5, each with 100 feature maps \citep{kimConvolutionalNeuralNetworks2014}. 
The model uses 100-dimensional word embeddings, which are initialized with pre-trained gloVe vectors \citep{penningtonGloveGlobalVectors2014}.

For the BERT utterance encoder, we use the BERT\textsubscript{BASE} model with hidden size of 768 and 12 transformer layers and self-attention heads \citep[][\S3.1]{devlinBERTPretrainingDeep2018}.
In our implementation, we use the un-cased model provided by \citet{wolfHuggingFaceTransformersStateoftheart2019}.

\section{Experiments}
\subsection{Experiment 1: Impact of laughter} \label{sec:experiment1}   % Vlad
In the first experiment we investigated whether laughter, as an example of a dialogue-specific signal, is a helpful feature for DAR.
Therefore, we train another version of each model: one containing laughs (\texttt{L}) and one with laughs left out (\texttt{NL}), and compare their performances in DAR task.
Table~\ref{table:laughter-total-acc} compares the results from applying the models with two different utterance encoders (\texttt{BERT}, \texttt{CNN}).
First of all, we can see that \texttt{BERT} utterance encoder outperforms \texttt{CNN} in all the cases.
From observing the effect of laughters, we can see differences in performance depending on the dialogue act, regardless how often laughter occurs in the current or adjacent utterances (see Figure~\ref{fig:swda-by-da} for Switchboard and Figure~\ref{fig:ami-by-da} for AMI corpus).
The strongest evidence for laughter as a helpful feature was found in SWDA for the \texttt{BERT} utterance encoder, where macro-F1 score increases by 7.89 percentage points.

\begin{table}
  \centering
  \begin{tabular}{@{}lcccc@{}}
    \toprule
                      & \multicolumn{2}{c}{SWDA} & \multicolumn{2}{c}{AMI-DA} \\ \midrule
                      & macro-F1 & acc. & macro-F1 & acc.       \\ 
    \texttt{BERT-NL}  & 38.10 & 77.07 & 49.09 & 67.06       \\ 
    \texttt{BERT-L}   & 45.99 & 76.93 & 50.17 & 67.12       \\ \midrule
    \texttt{CNN-NL}   & 37.23 & 75.08 & 38.37 & 63.46        \\
    \texttt{CNN-L}    & 27.59 & 75.40 & 37.94 & 64.30        \\ \bottomrule
    
  \end{tabular}
  \caption{Comparison of models depending on using laughter on the training phase. }
  \label{table:laughter-total-acc}
\end{table}

% \begin{table}
%   \label{tab:laughter}
%   \centering
%   \begin{tabular}{@{}lccc@{}}
%     \toprule
%     & \%lassoc & \%L  & $\Delta$ (mean acc.) \\ \midrule
%     Forward-CF & 11.86 & 4.59 & 11.40\\
%     Backward-CF & 11.57 & 3.03 & 4.30\\
%     Other & 12.71 & 4.40 & 2.39\\
%     CS & 20.63 & 14.17 & -1.50\\\bottomrule

%   \end{tabular}
%   \caption{Laughter impact by dialogue act group (SWDA, BERT), mean accuracy}
% \end{table}


In order to further investigate the differences, we looked at the dialogue act groups.
Mapping all dialogue acts into four groups allows direct comparison between model performances in SWDA and AMI-DA.
Table~\ref{table:laughter-group-acc} compares accuracies between the two main groups: Forward-Communicative-Function and Backward-Communicative-Function (82.3\% of all DAs in SWDA and 76.7\% in AMI-DA).
Our models tend to perform better for Forward-Communicative-Function group irrespective of presence the corpus and the presence of laughs.
Also, our models perform better on the AMI-DA corpus.
Overall performance in these groups appears to be unaffected by presence of laughs.

\begin{table}
  \centering
  \begin{tabular}{@{}llcc@{}}
    \toprule
    &                  & Forward-CF  & Backwards-CF   \\ \midrule
    SWDA &\texttt{BERT-NL}  & 92.54 & 91.67       \\ 
         &\texttt{BERT-L}   & 95.42 & 90.91       \\ 
         &\texttt{CNN-NL}   & 93.30 & 90.85        \\
         &\texttt{CNN-L}    & 94.64 & 90.13        \\ \midrule
    AMI  &\texttt{BERT-NL}  & 87.77 & 80.83       \\ 
         &\texttt{BERT-L}   & 87.36 & 79.87       \\
         &\texttt{CNN-NL}   & 91.85 & 71.21        \\
         &\texttt{CNN-L}    & 91.25 & 71.28        \\    \bottomrule
  \end{tabular}
  \caption{Comparison of models depending on using laughter on the training phase. }
  \label{table:laughter-group-acc}
\end{table}

% to appendix?
\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{img/SWDA-bertLvsNL.pdf}
  \caption{Change in accuracy for each SWDA dialogue act. Positive changes when adding laughter are shown in blue. Vertical bars indicate how often dialogue act is associated with laughter.}
    \label{fig:swda-by-da}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{img/AMI-DA-bertLvsNL.pdf}
  \caption{Change in accuracy for each AMI dialogue act. Positive changes when adding laughter are shown in blue. Vertical bars indicate how often dialogue act is associated with laughter.}
    \label{fig:ami-by-da}
  \end{figure*}

  

%
% TODO per dialogue act analysis
% DONE compare impacts of CNN vs BERT
% DONE for both AMI and SWDA
% ? 

\subsection{Experiment 2: Impact of pre-training vs. fine-tuning} \label{sec:experiment2} % Bill
Next, we analyze how pre-training affects BERT's performance as an utterance encoder.
To do so, we consider the performance of DAR models with three different utterance encoders:
\begin{itemize}
  \item \texttt{BERT-FT} -- pre-trained BERT with DAR fine-tuninig 
  \item \texttt{BERT-RI} -- randomly initialized BERT (with DAR fine-tuning)
  \item \texttt{BERT-FZ} -- pre-trained BERT without fine-tuning (frozen during DAR training)
\end{itemize}

\begin{table}[]
\begin{tabular}{@{}clll@{}}
\toprule
                                                                                  &             & SWDA  & AMI-DA \\ \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Micro-average\\ accuracy\end{tabular}} & \texttt{BERT-FT}     & 76.93 & 66.94  \\
                                                                                  & \texttt{BERT-RI}     & 73.80 & 61.53  \\ 
                                                                                  & \texttt{BERT-FZ}     & 55.61 & 46.59  \\ \midrule
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Macro-average\\ F1\end{tabular}}     & \texttt{BERT-FT}     & 45.99 & 50.03  \\
                                                                                  & \texttt{BERT-RI}     & 32.18 & 33.45  \\ 
                                                                                  & \texttt{BERT-FZ}     & 07.75 & 14.44  \\ \bottomrule
\end{tabular}
  \caption{DAR performance comparison of BERT with standard pre-training and DAR fine-tuning (\texttt{BERT-FT}) vs. the same model without pre-training (\texttt{BERT-RI}) and without fine-tuning (\texttt{BERT-FZ}).}
  \label{table:exp2}
\end{table}

The pre-trained model is more accurate than the randomly initialized model by several percentage points on both DA corpora,
suggesting that BERT's extensive pre-training does provide some useful information for recoginizing different dialogue acts (table \ref{table:exp2}).
This performance boost is much more pronounced in the macro-averaged F1 score,
which is explained by the fact that at the tag level, pre-training has a larger impact on less frequent tags. 

A key aspiration of transfer learning is to expose the model to phenomena that are too infrequent to learn from labeled training data alone.
One explanation for pre-trained BERT's superior performance on some infrequent dialogue acts is that it has learned to represent relevant features
that the randomly initialized model was unable to learn due to their sparsity in the labeled training set.
Indeed, a simple lexical probe supports this explanation: in utterances where the pre-trained model is correct and the randomly initialized model is not,
the rarest word is 1.9 times rarer than is typical of corpus as a whole.

Comparing the performance of the fine-tuned encoder to that of the frozen model, we note that pre-training alone is not enough.
The frozen model is heavily biased towards the most frequent tags, which explains its especially poor macro-average score (table \ref{table:exp2}).
In SWDA, for example, the model with a frozen encoder predicts one of the two most common tags (Statement-non-opinion or Acknowledge) \%86 of the time, whereas those two
tags account for only \%51 of the ground truth tags.
The fine-tuned model is much less biased; it predicts the two most common tags only \%59 of the time.

%These observations raise two questions.
%First, how does BERT's pre-training help with DAR? 
%And second, in what way are the representations learned by pre-trained BERT lacking?
%In other words, what is the contribution of fine-tuning?

%To help answer these questions, we compare performance of the above three models by dialogue act tag.

% TODO: add per dialogue act accuracy figures

\paragraph{Fine-tuning and laughter}
Since the laughter token doesn't appear in the original BERT vocabulary, only fine-tuned models can meaningfully make use of laughter in utterance representations. 
This gives us another opportunity to assess whether laughter is useful in dialogue act recognition.
We find that although \%4.6 of utterances in SWDA contain laughter, 
\%7.3 of utterances misclassified by the frozen model but correctly classified by the fine-tuned model contain laughter, 
suggesting that the fine-tuned encoder makes use of laughter. 
For AMI-DA, the effect is less pronounced, but still present: 
overall, \%8.5 of utterances contain laughter, 
compared to \%9.6 of utterances where fine-tuning makes a difference. 

\subsection{Experiment 3: Impact of dialogue pre-training} \label{sec:experiment3} % Bill

Next, we assess the effect of additional in-domain pre-training on BERT's performance as an utterance encoder.\footnote{
In-domain pre-training is sometimes referred to as \textit{fine-tuning}, but we reserve that term for task-specific training on labeled data.}
We construct pre-training corpora from the SWDA portion of the un-labeled Switchboard corpus and from the entire AMI corpus (including the 32 dialogues with no human-annotated DA tags that are not included in the DAR training set).
In both cases, we exclude dialogues that are reserved for DAR testing.

We use the same pre-training task described by \citet{devlinBERTPretrainingDeep2018}, which combines masked token prediction and next sentence (utterance) detection. 
Distractor utterances are drawn at random from another dialogue in the corpus
and each epoch is generated separately so that different distractor sentences appear each time.
We pre-trained three BERT models: with Switchboard (10 epochs), AMI (10 epochs), and the combined corpus (5 epochs).

For each dialogue act corpus, we trained DAR models with BERT encoders pre-trained on the in-domain corpus (\texttt{ID}) and the combined corpus (\texttt{CC}).
As before, we experimented with both the fine-tuning and frozen conditions.

The results are mixed.
In-domain pre-training offers a modest boost in overall accuracy,
but the macro-average scores are almost uniformly worse (table \ref{table:exp3}).
%Indeed, when BERT is frozen during fine-tuning, the model that received no additional pre-training performs better by more than 3 percentage points (table \ref{tab:exp3-recall}).
To fully assess the potential impact of in-domain pre-training, a larger dialogue corpus is required.

\begin{table}
\begin{tabular}{llrr}
\toprule
               &         &  SWBD &   AMI \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Micro-average\\ accuracy\end{tabular}} 
               & \texttt{FT} & 76.93 & 66.95 \\
               & \texttt{+ID-FT} & 77.02 & 68.66 \\
               & \texttt{+CC-FT} & 77.35 & 68.58 \\ \cline{2-4}
               & \texttt{FZ} & 55.61 & 46.60 \\
               & \texttt{+ID-FZ} & 52.30 & 48.07 \\
               & \texttt{+CC-FZ} & 51.14 & 42.42 \\ \hline
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Macro-average\\ F1 score\end{tabular}} 
               & \texttt{FT} & 45.99 & 50.03 \\
               & \texttt{+ID-FT} & 45.48 & 46.56 \\
               & \texttt{+CC-FT} & 47.78 & 48.72 \\ \cline{2-4}
               & \texttt{FZ} &  7.75 & 14.44 \\
               & \texttt{+ID-FZ} &  6.46 & 14.43 \\
               & \texttt{+CC-FZ} &  5.76 & 12.56 \\
               
\bottomrule
\end{tabular}
  \caption{Comparing DAR performance of  with additional in-domain (\texttt{ID}) and cross-domain (\texttt{CC}) dialogue pre-training,
    for both the frozen (\texttt{FZ}) and fine-tuned (\texttt{FT}) conditions.}
  \label{table:exp3}
\end{table}



%\subsection{Experiment 4: Cross-domain pre-training (if time permits)} \label{sec:experiment4} % Bill
%  - cross-domain pre-training
\section{Related work} % add references first
For the background:

Dialogue act recoginition
\begin{itemize}
  \item \citet{botheContextbasedApproachDialogue2018} -- uses an RNN for context (recently SOTA on SWDA)
  \item \citet{pragstVectorRepresentationUtterances2018} -- dialogue vector models. Uses vector-manual DA tag correspondance to argue for semantic relevance of their vector models (related to how we use DA tagging to verify BERT usefulness)
  \item \citet{cerisaraEffectsUsingWord2vec2017} -- standard pre-trained word embeddings do not help for DAR in any of three tested languages (English, French, Czech)
  \item \citet{kalchbrennerRecurrentConvolutionalNeural2013} -- DAR model similar to our architecture. Uses an RNN ``discourse model'' and a CNN similar to our baseline encoder
  \item \citet{khanpourDialogueActClassification2016} -- Deep LSTM utt representation. No notion of context. Finds word vectors are useful.
  \item \citet{tranPreservingDistributionalInformation2017} -- Sequential DA prediction with uncertainty propegation
  \item \citet{chenDialogueActRecognition2017} -- SOTA? SWDA DAR using attention and CRF over utterances
  \item \citet{ortegaNeuralbasedContextRepresentation2017} -- SWDA DAR with attention-based context representations and CNN utterance representations
  \item \citet{shenNeuralAttentionModels2016} -- Uses attention for utt encoding in DAR. No notion of context, I think.
  \item \citet{tranHierarchicalNeuralModel2017} -- RNNs at both the utteranec and dialogue level. Also attention. Very similar to our architecture.
\end{itemize}

Pre-training/transfer learning for dialogue
\begin{itemize}
  \item \citet{mehriStructuredFusionNetworks2019} -- ``Structured fusion networks''. Traditional dialogue system design with neural models.
  \item \citet{chenSemanticallyConditionedDialog2019a} -- uses BERT for response generation
  \item \citet{baoPLATOPretrainedDialogue2019} -- pre-training for dialogue generation
  \item \citet{mehriPretrainingMethodsDialog2019} -- looks at various dialogue tasks using pre-training, including DAR and using BERT
  \item \citet{vigComparisonTransferLearningApproaches} -- Transfer learning for response selection
\end{itemize}


\section{Discussion} % add items to discuss
\begin{itemize}
  \item RoBERTa training scheme finds that next sentence prediction is of little importance. Is this true for us too? (suspect not due to discourse importance). see: https://github.com/huggingface/transformers/issues/1622
  \item Issues related to working with transcribed speech: Different kinds of laughter are all represented by the same token. Does transcription orthography capture prosody? We know prosidy is useful for DAR (Jurafsky).
  \item future work: pre-training on very large un-labeled dialogue corpora. How does text chat transfer to the spoken/transcribed setting?
\end{itemize}

\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\end{document}
