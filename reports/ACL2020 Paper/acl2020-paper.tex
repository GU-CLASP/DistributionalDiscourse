%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{booktabs}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for ACL 2020 Proceedings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Basically we want to see if language models pre-trained on text data are useful for dialogue.
  So we look at if BERT is useful for dialouge act recoginiton.
  But we want to know if it's really adapting to the domain, so we look at how it uses laughter, a phenomenon specific to dialogue.
\end{abstract}

% INTRODUCTION

Recently, large-scale language models, trained on massive corpora of text data, have achieved state-of-the-art results on a variety of traditonal NLP tasks.
We investigate whether such models might be useful for processing dialogue, and if so, whether they adapt to make use of dialogue-specific features.

Dialogue, especially spoken dialogue, is radically different from the kind of data that neural language models are prestrained on.
Not only is the internal structure of contributions different---with features such as disfluencies, repair, incomplete sentences, and various vocal sounds---but the sequential structure of the discourse is also very different.
In dialouge speakers take turns and switch perpsectives.
More generally, the way that utterances contribute and cohere to the discourse is different from the relationship between sentences in a piece of written text.

One important dialogical feature missing from text data is laughter.
In the Switchboard dialogue corpus, laughter appears in approximately \%X of utterances.
Laughter also relates to the discourse structure of dialogue. 
Laughables.
Also useful for predicting DAs (is that already a result?)

In this work, we investigate whether BERT, a well-know pre-trained language model, is useful for dialogue.
We use dialogue act recoginition as a proxy task, since both the interal content of conributions, and their sequential structure have bearing on the task.

\paragraph{Fine-tuning and laughter}
Since the laughter token doesn't appear in the original BERT vocabulary, only fine-tuned models can meaningfully make use of laughter in utterance representations. 
This gives us another opportunity to assess whether laughter is useful in dialogue act recognition.
We find that although \%4.6 of utterances in SWDA contain laughter, \%7.3 of utterances misclassified by \texttt{BERT} but correctly classified by \texttt{BERT-FT} contain laughter, suggesting that the fine-tuned model utterance encoder is making use of laughter. 
For AMI-DA, the effect is less pronounced, but still presentÂ§: \%8.5 of utterances contain laughter, and \%9.6 of utterances where fine-tuning makes a difference contain laughter.


\section{Background} % Vlad
%  - dialogue acts
%  - laughter
%  - BERT 

References
\begin{itemize}
  \item \citet{austinHowThingsWords2009} -- speeh acts
  \item \citet{coreCodingDialogsDAMSL1997} -- DAMSL dialogue act tagging scheme. SWDA tags are based on this (I think AMI are too). Concept of multi-layer DA tags; forward/backward function
  \item \citet{jurafskySwitchboardSWBDDAMSLShallowDiscourseFunction1997a} -- SWBD-DAMSL coders manual
  \item \citet{devlinBERTPretrainingDeep2018} -- Original BERT paper
  \item \citet{sunHowFineTuneBERT2019} -- recommendations for fine-tuninig BERT
  \item \citet{mikolovDistributedRepresentationsWords2013} -- word2vec
  \item \citet{petersTuneNotTune2019} -- when and how to fine-tune pre-trained representations
  \item \citet{kimConvolutionalNeuralNetworks2014} -- CNN sentence representation
\end{itemize}

\section{Related work} % add references first
Dialogue act recoginition
\begin{itemize}
  \item \citet{pragstVectorRepresentationUtterances2018} -- dialogue vector models. Uses vector-manual DA tag correspondance to argue for semantic relevance of their vector models (related to how we use DA tagging to verrify BERT usefulness)
  \item \citet{cerisaraEffectsUsingWord2vec2017} -- standard pre-trained word embeddings do not help for DAR in any of three tested languages (English, French, Czech)
  \item \citet{kalchbrennerRecurrentConvolutionalNeural2013} -- DAR model similar to our architecture. Uses an RNN ``discourse model'' and a CNN similar to our baseline encoder
  \item \citet{stolckeDialogueActModeling2000} -- OG DAR model. Uses an HMM with various lexical and prosodic features
  \item \citet{khanpourDialogueActClassification2016} -- Deep LSTM utt representation. No notion of context. Finds word vectors are useful.
  \item \citet{tranPreservingDistributionalInformation2017} -- Sequential DA prediction with uncertainty propegation
  \item \citet{chenDialogueActRecognition2017} -- SOTA? SWDA DAR using attention and CRF over utterances
  \item \citet{ortegaNeuralbasedContextRepresentation2017} -- SWDA DAR with attention-based context representations and CNN utterance representations
  \item \citet{shenNeuralAttentionModels2016} -- Uses attention for utt encoding in DAR. No notion of context, I think.
  \item \citet{tranHierarchicalNeuralModel2017} -- RNNs at both the utteranec and dialogue level. Also attention. Very similar to our architecture.
\end{itemize}

Pre-training/transfer learning for dialogue
\begin{itemize}
  \item \citet{mehriStructuredFusionNetworks2019} -- ``Structured fusion networks''. Traditional dialogue system design with neural models.
  \item \citet{chenSemanticallyConditionedDialog2019a} -- uses BERT for response generation
  \item \citet{baoPLATOPretrainedDialogue2019} -- pre-traininig for dialogue generation
  \item \citet{mehriPretrainingMethodsDialog2019} -- looks at various dialogue tasks using pre-training, including DAR and using BERT
  \item \citet{vigComparisonTransferLearningApproaches} -- Transfer learning for response selection
\end{itemize}


\section{Data}
We perform experiments on the Switchboard Dialogue Act Corpus (SWDA), which is a subset of the larger Switchboard corpus, and the dialogue act-tagged portion of the AMI Meeting Corpus (AMI-DA).

% - DA distribution              % Vlad
% - Laughter frequencies per DA  % Vlad

% - AMI, SWBD + comparison table % Bill
\begin{table}[]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Switchboard}       & \textbf{AMI Corpus}                     \\ \midrule
Dyadic                     & Multi-party                             \\
Casual conversation        & Mock business meeting                   \\
Telephone                  & In-person \& video                      \\ \midrule
English                    & English                                 \\ 
Native speakers            & Native \& non-native speakers           \\ 
early '90s                 & 2000s                                   \\ \midrule
2200 conversations         & 171 meetings                            \\
  \hspace{1em} 1155 in SWDA               & \hspace{1em} 139 in AMI-DA                           \\
400 utterances             & 118k utterances                         \\
3M tokens                  & 1.2M tokens                             \\ \bottomrule
\end{tabular}
  \caption{Comparison between Switchboard and the AMI Meeting Corpus}
  \label{table:corpora}
\end{table}

% - Preprocessing: remove disfluencies, acronyms and speaker tokens in AMI, removing laughter % Bill
  
\section{Model} % Bill
%  - DAR model -> utt encoder, RNN sequence tagger
%  - BERT encoder (12 heads, uncased,...)
%  - baseline encoder

\section{Experiments}
\subsection{Experiment 1: Impact of laughter}   % Vlad
% per dialogue act analysis
% compare impacts of CNN vs BERT
% for both AMI and SWDA

\subsection{Experiment 2: Impact of pre-training} % Bill
% - compare i) pre-trained BERT with freezing ii) pretrained with no freezing iii) randolmly initialized
Next, we take a direct look at how pre-training affects BERT's performance as an utterance encoder for our DAR model.
In particular, we consider the performance of DAR models with three different utterance encoders:
\begin{itemize}
  \item \texttt{BERT} -- pre-trained BERT without fine-tuning (frozen during DAR training)
  \item \texttt{BERT-FT} -- pre-trained BERT with fine-tuninig 
  \item \texttt{BERT-RI} -- randomly initialized BERT with fine-tuning 
\end{itemize}

\begin{table}[]
\centering 
\begin{tabular}{@{}lll@{}}
\toprule
                 & SWDA  & AMI-DA \\ \midrule
\texttt{BERT}       & 55.61 & 46.59  \\
\texttt{BERT-FT}    & 76.93 & 66.94  \\
\texttt{BERT-RI}    & 73.80 & 61.53  
\end{tabular}
  \caption{Micro-average DAR accuracy}
  \label{table:exp2-avg}
\end{table}    

The pre-trained model performs better than the randomly initialized model by several percentage points on both DA corpora (table \ref{table:exp2-avg}). 
These results suggest that BERT's extensive pre-training do provide some useful information for dialogue act recoginition.
However, we note that pre-training is not enough: the poor performance of the frozen \texttt{BERT} model indicates that fine-tuning is important for BERT's performance as an utterance encoder.
These observations raise two questions.
First, how does BERT's pre-training help with DAR? 
And second, in what way are the representations learned by pre-trained BERT lacking?
In other words, what is the contribution of fine-tuning?

To help answer these questions, we compare accuracy results of the above three models by dialogue act tag.

% TODO: add per dialogue act accuracy figures

\subsection{Experiment 3: Impact of dialogue pre-training} % Bill
% - pre-train AMI+SWBD -> train SWDA/AMI-DA -> test SWDA/AMI-DA
% - pre-traing tasks: 1) next utterance, 2) masked token, 3) both ?
% - compare frozen with no dialogue pre-training vs pre-trained on dialogue w.r.t. L/NL (same as in Experiment 1)
\begin{itemize}
  \item Setup
    \begin{itemize}
      \item we do additional pre-training for the BERT encoder on in-domain data
      \item note: this is sometimes referred to as fine-tuning, but we reserve that term for task-specific training of the encoder model

      \item SWDA/AMI (excluding dialogues in the DAR test set)
      \item 10 epoch AMI / 10 epochs SWBD / 5 epochs of each
    \end{itemize}
\end{itemize}

\subsection{Experiment 4: Cross-domain pre-training (if time permits)} % Bill
%  - cross-domain pre-training

\section{Discussion} % add items to discuss
\begin{itemize}
  \item RoBERTa training scheme finds that next sentence prediction is of little importance. Is this true for us too? (suspect not due to discourse importance). see: https://github.com/huggingface/transformers/issues/1622
\end{itemize}

\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\end{document}
