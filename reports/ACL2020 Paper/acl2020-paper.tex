%
% File acl2020.tex
%
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2020}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{natbib}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Instructions for ACL 2020 Proceedings}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  Basically we want to see if language models pre-trained on text data are useful for dialogue.
  So we look at if BERT is useful for dialogue act recognition.
  But we want to know if it's really adapting to the domain, so we look at how it uses laughter, a phenomenon specific to dialogue.
\end{abstract}

% INTRODUCTION

Recently, large-scale language models, trained on massive corpora of text data, have achieved state-of-the-art results on a variety of traditional NLP tasks.
We investigate whether such models might be useful for processing dialogue, and if so, whether they adapt to make use of dialogue-specific features.

Dialogue, especially spoken dialogue, is radically different from the kind of data that neural language models are prestrained on.
Not only is the internal structure of contributions different---with features such as disfluencies, repair, incomplete sentences, and various vocal sounds---but the sequential structure of the discourse is also very different.
In dialogue speakers take turns and switch perspectives.
More generally, the way that utterances contribute and cohere to the discourse is different from the relationship between sentences in a piece of written text.

One important dialogical feature missing from text data is laughter.
In the Switchboard dialogue corpus, laughter appears in approximately \%X of utterances.
Laughter also relates to the discourse structure of dialogue. 
% Laughables.
Also useful for predicting DAs (is that already a result?)

In this work, we investigate whether BERT \citep{devlinBERTPretrainingDeep2018}, a well-know pre-trained language model, is useful for dialogue.
We use dialogue act recoginition as a proxy task, since both the interal content of conributions, and their sequential structure have bearing on the task.

\paragraph{Contributions}
The main results of this paper have to do with to the usefulness of language model fine-tuning for DAR, 
the impact of laughter on DAR, and the relationship between the two.
\begin{itemize}
  \item Laughter is useful for dialogue act recoginition (\S\ref{sec:experiment1}).
  \item The impact of laughter is different for different dialogue acts. [Bigger for forward/backward?] (\S\ref{sec:experiment1})
  \item Standard BERT pre-traininig is useful for DAR, but the impact of pre-training is smaller than than of fine-tuning (\S\ref{sec:experiment2})
  \item 
\end{itemize}


\section{Background} % Vlad
%  - dialogue acts
%  - laughter
%  - BERT 

References
\begin{itemize}
  \item \citet{austinHowThingsWords2009} -- speeh acts
  \item \citet{coreCodingDialogsDAMSL1997} -- DAMSL dialogue act tagging scheme. SWDA tags are based on this (I think AMI are too). Concept of multi-layer DA tags; forward/backward function
  \item \citet{jurafskySwitchboardSWBDDAMSLShallowDiscourseFunction1997a} -- SWBD-DAMSL coders manual
  % \item \citet{devlinBERTPretrainingDeep2018} -- Original BERT paper 
  \item \citet{sunHowFineTuneBERT2019} -- recommendations for fine-tuninig BERT
  \item \citet{mikolovDistributedRepresentationsWords2013} -- word2vec
  \item \citet{petersTuneNotTune2019} -- when and how to fine-tune pre-trained representations
  \item \citet{kimConvolutionalNeuralNetworks2014} -- CNN sentence representation
  \item \citet{stolckeDialogueActModeling2000} -- OG DAR model. Uses an HMM with various lexical and prosodic features
\end{itemize}

\section{Data}
We perform experiments on the Switchboard Dialogue Act Corpus (SWDA), which is a subset of the larger Switchboard corpus, and the dialogue act-tagged portion of the AMI Meeting Corpus (AMI-DA). 

% - DA distribution              % Vlad

\begin{table}[]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Switchboard}       & \textbf{AMI Corpus}                     \\ \midrule
Dyadic                     & Multi-party                             \\
Casual conversation        & Mock business meeting                   \\
Telephone                  & In-person \& video                      \\ \midrule
English                    & English                                 \\ 
Native speakers            & Native \& non-native speakers           \\ 
early '90s                 & 2000s                                   \\ \midrule
2200 conversations         & 171 meetings                            \\
  \hspace{1em} 1155 in SWDA               & \hspace{1em} 139 in AMI-DA                           \\
400k utterances             & 118k utterances                         \\
3M tokens                  & 1.2M tokens                             \\ \bottomrule
\end{tabular}
  \caption{Comparison between Switchboard and the AMI Meeting Corpus}
  \label{table:corpora}
\end{table}

The distribution of laughs in different dialogue acts has a rather uniform shape with a few outliers (see box plots). Figures \ref{fig:swda-by-da} (for SWDA) and \ref{fig:ami-by-da} (for AMI-DA) provide a breakdown of dialogue act depending on whether current or adjacent utterances (the preceding utterance and the following one are counted as adjacent) contain a laughter. Note that SWDA has a ``Nonverbal'' dialogue act which is misguiding with respect to laughter, because utterances only containing a single laughter token fall into this category, however they can serve, for example, to acknowledge a statement or to deflect a question  \citep{mazzocconi2019phd}.

\begin{figure}
  \centering
  \includegraphics[width=\the\columnwidth]{img/box-swda.pdf}
  \caption{Box plot for utterances associated with laughter for SWDA. On the left: proportion of DAs having laughter in one of the adjacent utterances, on the right: proportion of DAs containing laughter. }
    \label{fig:box-swda}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\the\columnwidth]{img/box-ami.pdf}
  \caption{Box plot for utterances associated with laughter for AMI-DA. On the left: proportion of DAs having laughter in one of the adjacent utterances, on the right: proportion of DAs containing laughter. }
    \label{fig:box-ami}
\end{figure}

For SWDA corpus we also considered grouping the dialogue acts according to DAMSL coders manual \citep{jurafskySwitchboardSWBDDAMSLShallowDiscourseFunction1997a}: Forward-Communicative-Function (51.86\% of all DAs), Backwards-Communicative-Function (30.35\%), Other (9.04\%) and Communicative-Status (8.75\%). We observed even distribution of laughs in these groups, except the Communicative-Status group which contains ``Nonverbal'' dialogue acts.

% - Preprocessing: remove disfluencies, acronyms and speaker tokens in AMI, removing laughter % Bill
  
\section{Model} % Bill

We employ a simple DAR model with two components: an encoder that vectorizes utterances, and a sequence model that predicts dialogue act tags from the vectorized utterances (figure \ref{fig:model-architecture}).
Since we are primarily interested in comparing different utterance encoders, we use a simple RNN as the sequence model in every configuration. 
The RNN takes the encoded utterance as input at each time step
Its hidden state is passed to a simple linear classification layer over dialogue act tags.
Conceptually, the encoded utterance represents the context-agnostic semantic content of the utterance, and the hidden state of the RNN represents the discourse context.

\begin{figure*}
  \include{model}
  \caption{Simple neural dialogue act recognition sequence model}
  \label{fig:model-architecture}
\end{figure*}

As a baseline utterance encoder, we use a word-level CNN with window sizes of 3, 4, and 5, each with 100 feature maps \citep{kimConvolutionalNeuralNetworks2014}. 
The model uses 100-dimensional word embeddings, which are initialized with pre-trained gloVe vectors in some configurations \citep{penningtonGloveGlobalVectors2014}.

For the BERT utterance encoder, we use the BERT-BASE model with hidden size of 768 and 12 transformer layers and self-attention heads \citep[][see \S3.1]{devlinBERTPretrainingDeep2018}.
We use the un-cased model implemented by \citet{wolfHuggingFaceTransformersStateoftheart2019}.

\section{Experiments}
\subsection{Experiment 1: Impact of laughter} \label{sec:experiment1}   % Vlad
In the first experiment we investigated whether laughter, as an example of a dialogue-specific signal, is a helpful feature for DAR.
Therefore, we train another version of each model: one containing laughs (\texttt{L}) and one with laughs left out (\texttt{NL}), and compare their performances in DAR task.
Table~\ref{table:laughter-total-acc}) compares the results from applying the models with two different utterance encoders (\texttt{BERT}, \texttt{CNN}). From observing the total accuracy scores, models appear to be almost unaffected by leaving out the laughters. On the other hand, we can observe differences in performance depending on the dialogue act, regardless how often laughter occurs in the current or adjacent utterances (see Figure~\ref{fig:swda-by-da} for Switchboard and Figure~\ref{fig:ami-by-da} for AMI corpus).

In order to further investigate the differences, we looked at the dialogue act groups.
\begin{table}
  \centering
  \begin{tabular}{@{}lll@{}}
    \toprule
                      & SWDA  & AMI-DA   \\ \midrule
    \texttt{BERT-NL}  & 77.07 & 67.06       \\ 
    \texttt{BERT-L}   & 76.93 & 67.12       \\ \midrule
    \texttt{CNN-NL}   & 75.08 & 63.46        \\
    \texttt{CNN-L}    & 75.40 & 64.30        \\ \bottomrule
    
  \end{tabular}
  \caption{Comparison of models depending on using laughter on the training phase. }
  \label{table:laughter-total-acc}
\end{table}

% \begin{table}
%   \label{tab:laughter}
%   \centering
%   \begin{tabular}{@{}lccc@{}}
%     \toprule
%     & \%lassoc & \%L  & $\Delta$ (mean acc.) \\ \midrule
%     Forward-CF & 11.86 & 4.59 & 11.40\\
%     Backward-CF & 11.57 & 3.03 & 4.30\\
%     Other & 12.71 & 4.40 & 2.39\\
%     CS & 20.63 & 14.17 & -1.50\\\bottomrule

%   \end{tabular}
%   \caption{Laughter impact by dialogue act group (SWDA, BERT), mean accuracy}
% \end{table}




\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{img/SWDA-bertLvsNL.pdf}
  \caption{Change in accuracy for each SWDA dialogue act. Positive changes when adding laughter are shown in blue. Vertical bars indicate how often dialogue act is associated with laughter.}
    \label{fig:swda-by-da}
\end{figure*}

\begin{figure*}
  \label{fig:ami-by-da}
  \centering
  \includegraphics[width=\textwidth]{img/AMI-DA-bertLvsNL.pdf}
  \caption{Change in accuracy for each AMI dialogue act. Positive changes when adding laughter are shown in blue. Vertical bars indicate how often dialogue act is associated with laughter.}
\end{figure*}
% TODO per dialogue act analysis
% DONE compare impacts of CNN vs BERT
% DONE for both AMI and SWDA
% ? 

\subsection{Experiment 2: Impact of pre-training vs. fine-tuning} \label{sec:experiment2} % Bill
Next, we analyze how pre-training affects BERT's performance as an utterance encoder.
To do so, we consider the performance of DAR models with three different utterance encoders:
\begin{itemize}
  \item \texttt{BERT-FT} -- pre-trained BERT with DAR fine-tuninig 
  \item \texttt{BERT-RI} -- randomly initialized BERT (with DAR fine-tuning)
  \item \texttt{BERT-FZ} -- pre-trained BERT without fine-tuning (frozen during DAR training)
\end{itemize}

\begin{table}[]
\begin{tabular}{@{}clll@{}}
\toprule
                                                                                  &             & SWDA  & AMI-DA \\ \midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Micro-average\\ accuracy\end{tabular}} & \texttt{BERT-FT}     & 76.93 & 66.94  \\
                                                                                  & \texttt{BERT-RI}     & 73.80 & 61.53  \\ 
                                                                                  & \texttt{BERT-FZ}     & 55.61 & 46.59  \\ \midrule
  \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Macro-average\\ F1\end{tabular}}     & \texttt{BERT-FT}     & 45.99 & 50.03  \\
                                                                                  & \texttt{BERT-RI}     & 32.18 & 33.45  \\ 
                                                                                  & \texttt{BERT-FZ}     & 07.75 & 14.44  \\ \bottomrule
\end{tabular}
  \caption{DAR performance comparison of BERT with standard pre-training and DAR fine-tuning (\texttt{BERT-FT}) vs. the same model without pre-training (\texttt{BERT-RI}) and without fine-tuning (\texttt{BERT-FZ}).}
  \label{table:exp2}
\end{table}

The pre-trained model is more accurate than the randomly initialized model by several percentage points on both DA corpora,
suggesting that BERT's extensive pre-training does provide some useful information for recoginizing different dialogue acts (table \ref{table:exp2}).
This performance boost is much more pronounced in the macro-averaged F1 score,
which is explained by the fact that at the tag level, pre-training has a larger impact on less frequent tags. 

A key asperation of transfer learning is to expose the model to phenomena that are too infrequent to learn from labeled training data alone.
One explanation for pre-trained BERT's superrior performance on some infrequent dialogue acts is that it has learned to represent relevant features
that the randomly initialized model was unable to learn due to their sparsity in the labeled training set.
Indeed, a simple lexical probe supports this explanation: in utterances where the pre-trained model is correct and the randomly initalized model is not,
the rarest word is 1.9 times rarer than is typcial of corpus as a whole.

Comparing the performance of the fine-tuned encoder to that of the frozen model, we note that pre-training alone is not enough.
The frozen model is heavily biased towards the most frequent tags, which explains its especially poor macro-average score (table \ref{table:exp2}).
In SWDA, the model with the frozen encoder predicts one of the two most common tags (Statement-non-opinion or Acknowledge) \%86 of the time, whereas those two
tags account for only \%51 of the ground truth tags.
The fine-tuned model is much less biased; it predicts the two most common tags only \%59 of the time.

%These observations raise two questions.
%First, how does BERT's pre-training help with DAR? 
%And second, in what way are the representations learned by pre-trained BERT lacking?
%In other words, what is the contribution of fine-tuning?

%To help answer these questions, we compare performance of the above three models by dialogue act tag.

% TODO: add per dialogue act accuracy figures

\paragraph{Fine-tuning and laughter}
Since the laughter token doesn't appear in the original BERT vocabulary, only fine-tuned models can meaningfully make use of laughter in utterance representations. 
This gives us another opportunity to assess whether laughter is useful in dialogue act recognition.
We find that although \%4.6 of utterances in SWDA contain laughter, 
\%7.3 of utterances misclassified by the frozen model but correctly classified by the fine-tuned model contain laughter, 
suggesting that the fine-tuned encoder makes use of laughter. 
For AMI-DA, the effect is less pronounced, but still present: 
overall, \%8.5 of utterances contain laughter, 
compared to \%9.6 of utterances where fine-tuning makes a difference. 

\subsection{Experiment 3: Impact of dialogue pre-training} \label{sec:experiment3} % Bill

Next, we assess the effect of additional in-domain pre-training on BERT's performance as an utterance encoder.\footnote{
In-domain pre-training is sometimes referred to as \textit{fine-tuning}, but we reserve that term for task-specific training on labeled data.}
We construct pre-training corpora from the SWDA portion of the un-labelled Switchboard corpus and from the entire AMI corpus (including the 32 dialogues with no human-annotated DA tags that are not included in the DAR training set).
In both cases, we exclude dialogues that are reserved for DAR testing.

We use the same pre-training task described by \citet{devlinBERTPretrainingDeep2018}, which combines masked token prediction and next sentence (utterance) detection. 
Distractor utterances are drawn at random from another dialogue in the corpus
and each epoch is generated separately so that different distractor sentences appear each time.
We pre-trained three BERT models: with Switchboard (10 epochs), AMI (10 epochs), and the combined corpus (5 epochs).

For each dialogue act corpus, we trained DAR models with BERT encoders pre-trained on the in-domain corpus (\texttt{ID}) and the comined corpus (\texttt{CC}).
As before, we experimented with both the fine-tuning and frozen conditions.

The results are mixed.
In-domain pre-training offers a modest boost in overall accuracy,
but the macro-average scores are almost uniformly worse (table \ref{table:exp3}).
%Indeed, when BERT is frozen during fine-tuning, the model that received no additional pre-training performs better by more than 3 percentage points (table \ref{tab:exp3-recall}).
To fully assess the potential impact of in-domain pre-training, a larger dialogue corpus is required.

\begin{table}
\begin{tabular}{llrr}
\toprule
               &         &  SWBD &   AMI \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Micro-average\\ accuracy\end{tabular}} 
               & \texttt{FT} & 76.93 & 66.95 \\
               & \texttt{+ID-FT} & 77.02 & 68.66 \\
               & \texttt{+CC-FT} & 77.35 & 68.58 \\ \cline{2-4}
               & \texttt{FZ} & 55.61 & 46.60 \\
               & \texttt{+ID-FZ} & 52.30 & 48.07 \\
               & \texttt{+CC-FZ} & 51.14 & 42.42 \\ \hline
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Macro-average\\ F1 score\end{tabular}} 
               & \texttt{FT} & 45.99 & 50.03 \\
               & \texttt{+ID-FT} & 45.48 & 46.56 \\
               & \texttt{+CC-FT} & 47.78 & 48.72 \\ \cline{2-4}
               & \texttt{FZ} &  7.75 & 14.44 \\
               & \texttt{+ID-FZ} &  6.46 & 14.43 \\
               & \texttt{+CC-FZ} &  5.76 & 12.56 \\
               
\bottomrule
\end{tabular}
  \caption{Comparing DAR performance of  with additional in-domain (\texttt{ID}) and cross-domain (\texttt{CC}) dialogue pre-training,
    for both the frozen (\texttt{FZ}) and fine-tuned (\texttt{FT}) conditions.}
  \label{table:exp3}
\end{table}



%\subsection{Experiment 4: Cross-domain pre-training (if time permits)} \label{sec:experiment4} % Bill
%  - cross-domain pre-training
\section{Related work} % add references first
Dialogue act recoginition
\begin{itemize}
  \item \citet{botheContextbasedApproachDialogue2018} -- uses an RNN for context (recently SOTA on SWDA)
  \item \citet{pragstVectorRepresentationUtterances2018} -- dialogue vector models. Uses vector-manual DA tag correspondance to argue for semantic relevance of their vector models (related to how we use DA tagging to verrify BERT usefulness)
  \item \citet{cerisaraEffectsUsingWord2vec2017} -- standard pre-trained word embeddings do not help for DAR in any of three tested languages (English, French, Czech)
  \item \citet{kalchbrennerRecurrentConvolutionalNeural2013} -- DAR model similar to our architecture. Uses an RNN ``discourse model'' and a CNN similar to our baseline encoder
  \item \citet{khanpourDialogueActClassification2016} -- Deep LSTM utt representation. No notion of context. Finds word vectors are useful.
  \item \citet{tranPreservingDistributionalInformation2017} -- Sequential DA prediction with uncertainty propegation
  \item \citet{chenDialogueActRecognition2017} -- SOTA? SWDA DAR using attention and CRF over utterances
  \item \citet{ortegaNeuralbasedContextRepresentation2017} -- SWDA DAR with attention-based context representations and CNN utterance representations
  \item \citet{shenNeuralAttentionModels2016} -- Uses attention for utt encoding in DAR. No notion of context, I think.
  \item \citet{tranHierarchicalNeuralModel2017} -- RNNs at both the utteranec and dialogue level. Also attention. Very similar to our architecture.
\end{itemize}

Pre-training/transfer learning for dialogue
\begin{itemize}
  \item \citet{mehriStructuredFusionNetworks2019} -- ``Structured fusion networks''. Traditional dialogue system design with neural models.
  \item \citet{chenSemanticallyConditionedDialog2019a} -- uses BERT for response generation
  \item \citet{baoPLATOPretrainedDialogue2019} -- pre-traininig for dialogue generation
  \item \citet{mehriPretrainingMethodsDialog2019} -- looks at various dialogue tasks using pre-training, including DAR and using BERT
  \item \citet{vigComparisonTransferLearningApproaches} -- Transfer learning for response selection
\end{itemize}


\section{Discussion} % add items to discuss
\begin{itemize}
  \item RoBERTa training scheme finds that next sentence prediction is of little importance. Is this true for us too? (suspect not due to discourse importance). see: https://github.com/huggingface/transformers/issues/1622
  \item Issues related to working with transcribed speech: Different kinds of laughter are all represented by the same token. Does transcription orthography capture prosidy? We know prosidy is useful for DAR (Jurafsky).
\end{itemize}

\bibliography{acl2020}
\bibliographystyle{acl_natbib}

\end{document}
