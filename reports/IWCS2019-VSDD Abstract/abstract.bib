
@inproceedings{coreCodingDialogsDAMSL1997,
  langid = {english},
  title = {Coding {{Dialogs}} with the {{DAMSL Annotation Scheme}}},
  abstract = {This paper describes the DAMSL annotation scheme for communicative acts in dialog. The scheme has three layers: Forward Communicative Functions, Backward Communicative Functions, and Utterance Features. Each layer allows multiple communicative functions of an utterance to be labeled. The Forward Communicative Functions consist of a taxonomy in a similar style as the actions of traditional speech act theory. The Backward Communicative Functions indicate how the current utterance relates to the previous dialog, such as accepting a proposal, con rming understanding, or answering a question. The Utterance Features include information about an utterance's form and content, such as whether an utterance concerns the communication process itself or deals with the subject at hand. The kappa inter-annotator reliability scores for the rst test of DAMSL with human annotators show promise, but are on average 0.15 lower than the accepted kappa scores for such annotations. However, the slight revisions to DAMSL discussed here should increase accuracy on the next set of tests and produce a reliable, exible, and comprehensive utterance annotation scheme.},
  booktitle = {Working {{Notes}} of the {{AAAI Fall Symposium}} on {{Communicative Action}} in {{Humans}} and {{Machines}}},
  date = {1997},
  year = {1997},
  pages = {28-35},
  author = {Core, Mark G and Allen, James F},
  file = {/Users/xnobwi/.zotero/storage/BKUWT57X/Core and Allen - Coding Dialogs with the DAMSL Annotation Scheme.pdf}
}

@inproceedings{pragstVectorRepresentationUtterances2018,
  title = {On the {{Vector Representation}} of {{Utterances}} in {{Dialogue Context}}},
  url = {http://aclweb.org/anthology/L18-1124},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}-2018)},
  publisher = {{European Language Resource Association}},
  urldate = {2019-03-12},
  date = {2018},
  year = {2018},
  keywords = {suggestion},
  author = {Pragst, Louisa and Rach, Niklas and Minker, Wolfgang and Ultes, Stefan},
  file = {/Users/xnobwi/.zotero/storage/3DEGXJPE/Pragst et al. - 2018 - On the Vector Representation of Utterances in Dial.pdf},
  venue = {Miyazaki, Japan}
}

@article{devlinBERTPretrainingDeep2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.04805},
  primaryClass = {cs},
  langid = {english},
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  url = {http://arxiv.org/abs/1810.04805},
  shorttitle = {{{BERT}}},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be ﬁne-tuned with just one additional output layer to create state-of-theart models for a wide range of tasks, such as question answering and language inference, without substantial task-speciﬁc architecture modiﬁcations.},
  urldate = {2019-04-04},
  date = {2018-10-10},
  year = {2018},
  keywords = {Computer Science - Computation and Language},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  file = {/Users/xnobwi/.zotero/storage/CD98FF2E/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@article{cerisaraEffectsUsingWord2vec2017,
  langid = {english},
  title = {On the Effects of Using Word2vec Representations in Neural Networks for Dialogue Act Recognition},
  volume = {47},
  issn = {08852308},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230816300456},
  doi = {10.1016/j.csl.2017.07.009},
  journaltitle = {Computer Speech \& Language},
  urldate = {2019-04-05},
  date = {2017},
  year = {2017},
  pages = {175-193},
  author = {Cerisara, Christophe and Král, Pavel and Lenc, Ladislav},
  file = {/Users/xnobwi/.zotero/storage/4WXBE8SF/1-s2.0-S0885230816300456-main.pdf}
}

@book{austinHowThingsWords2009,
  langid = {english},
  location = {{Cambridge, Mass}},
  title = {How to Do Things with Words: The {{William James}} Lectures Delivered at {{Harvard University}} in 1955},
  edition = {2. ed., [repr.]},
  isbn = {978-0-674-41152-4},
  shorttitle = {How to Do Things with Words},
  pagetotal = {168},
  publisher = {{Harvard Univ. Press}},
  date = {2009},
  year = {2009},
  author = {Austin, John L. and Urmson, James O.},
  file = {/Users/xnobwi/.zotero/storage/2MCXA76R/Austin - HOW TO DO THINGS WITH WORDS.pdf},
}

@inproceedings{kalchbrennerRecurrentConvolutionalNeural2013,
  langid = {english},
  title = {Recurrent {{Convolutional Neural Networks}} for {{Discourse Compositionality}}},
  abstract = {The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. The discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers. Without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classiﬁcation experiment.},
  booktitle = {Proceedings of the {{Workshop}} on {{Continuous Vector Space Models}} and Their {{Compositionality}}},
  date = {2013},
  year = {2013},
  pages = {119-126},
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  file = {/Users/xnobwi/.zotero/storage/SRUYIRW5/Kalchbrenner and Blunsom - Recurrent Convolutional Neural Networks for Discou.pdf}
}

@article{stolckeDialogueActModeling2000,
  langid = {english},
  title = {Dialogue {{Act Modeling}} for {{Automatic Tagging}} and {{Recognition}} of {{Conversational Speech}}},
  volume = {26},
  issn = {0891-2017, 1530-9312},
  url = {http://www.mitpressjournals.org/doi/10.1162/089120100561737},
  doi = {10.1162/089120100561737},
  number = {3},
  journaltitle = {Computational Linguistics},
  urldate = {2019-04-05},
  date = {2000-09},
  year = {2000},
  pages = {339-373},
  author = {Stolcke, Andreas and Ries, Klaus and Coccaro, Noah and Shriberg, Elizabeth and Bates, Rebecca and Jurafsky, Daniel and Taylor, Paul and Martin, Rachel and Ess-Dykema, Carol Van and Meteer, Marie},
  file = {/Users/xnobwi/.zotero/storage/E2RM82HV/Stolcke et al. - 2000 - Dialogue Act Modeling for Automatic Tagging and Re.pdf},
  year={2000}
}

@inproceedings{sordoniNeuralNetworkApproach2015,
  langid = {english},
  location = {{Denver, Colorado}},
  title = {A {{Neural Network Approach}} to {{Context}}-{{Sensitive Generation}} of {{Conversational Responses}}},
  url = {http://aclweb.org/anthology/N15-1020},
  doi = {10.3115/v1/N15-1020},
  abstract = {We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.},
  eventtitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-04-05},
  date = 2015,
  year = 2015,
  pages = {196-205},
  author = {Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, Bill},
  file = {/Users/xnobwi/.zotero/storage/6QR84V9B/Sordoni et al. - 2015 - A Neural Network Approach to Context-Sensitive Gen.pdf}
}

@article{khanpourDialogueActClassification,
  langid = {english},
  title = {Dialogue {{Act Classification}} in {{Domain}}-{{Independent Conversations Using}} a {{Deep Recurrent Neural Network}}},
  abstract = {In this study, we applied a deep LSTM structure to classify dialogue acts (DAs) in open-domain conversations. We found that the word embeddings parameters, dropout regularization, decay rate and number of layers are the parameters that have the largest effect on the ﬁnal system accuracy. Using the ﬁndings of these experiments, we trained a deep LSTM network that outperforms the state-of-the-art on the Switchboard corpus by 3.11\%, and MRDA by 2.2\%.},
  pages = {10},
  author = {Khanpour, Hamed and Guntakandla, Nishitha and Nielsen, Rodney},
  file = {/Users/xnobwi/.zotero/storage/79IVLLM8/Khanpour et al. - Dialogue Act Classification in Domain-Independent .pdf},
  booktitle={Proceedings of the 26th International Conference on Computational Linguistics},
  pages={2012--2021},
  year={2016}
}

@inproceedings{tranPreservingDistributionalInformation2017,
  langid = {english},
  location = {{Copenhagen, Denmark}},
  title = {Preserving {{Distributional Information}} in {{Dialogue Act Classification}}},
  url = {http://aclweb.org/anthology/D17-1229},
  doi = {10.18653/v1/D17-1229},
  abstract = {This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classiﬁcation demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively simple, it outperforms more complex neural models, achieving state-of-the-art results on the MapTask and Switchboard corpora.},
  eventtitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2019-04-05},
  date = {2017},
  year = {2017},
  pages = {2151-2156},
  author = {Tran, Quan Hung and Zukerman, Ingrid and Haffari, Gholamreza},
  file = {/Users/xnobwi/.zotero/storage/LDF2M2KJ/Tran et al. - 2017 - Preserving Distributional Information in Dialogue .pdf}
}

@inproceedings{hough2017joint,
  title={Joint, incremental disfluency detection and utterance segmentation from speech},
  author={Hough, Julian and Schlangen, David},
  booktitle={Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
  volume={1},
  pages={326--336},
  year={2017}
}

@article{shalyminov2018multi,
  title={Multi-Task Learning for Domain-General Spoken Disfluency Detection in Dialogue Systems},
  author={Shalyminov, Igor and Eshghi, Arash and Lemon, Oliver},
  journal={arXiv preprint arXiv:1810.03352},
  year={2018}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@inproceedings{chen2018dialogue,
  title={Dialogue act recognition via crf-attentive structured network},
  author={Chen, Zheqian and Yang, Rongqin and Zhao, Zhou and Cai, Deng and He, Xiaofei},
  booktitle={The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval},
  pages={225--234},
  year={2018},
  organization={ACM}
}

