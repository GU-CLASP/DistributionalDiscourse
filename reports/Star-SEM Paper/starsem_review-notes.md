* reviewer 1
- DAR is not as appropriate as NLU for laughter?? 
- fix:
  - change to the 1st edition of Austin
  - I also wouldn't say that conditional random fields are more
    "sophisticated" than most flavors of RNNs. -- clarify
- reformulate the contributions
- laughter in non-dialogue?
- why single symbol, fix in Data (ref ginzburg/chiara) but we have just a single type.
- why laughter is helpful? restate that in the discussion

idea: can laughter help with repair? 
* reviewer 2
- The opening two paragraphs get off on the wrong foot for me. The first sentence is too often repeated in papers like this, and the rest of these two paragraphs seems to under-state the extent to which there is prior work on using neural models for dialogue.
  - try to re-write first two para, more laughter and multi-modal dialogue
- label outliers on Fig 1
- say that non-verbal is not accounted for in the Task 1
- Fig 3. Plot: P(qw|qh) - P(qw) instead
- numbers for qh|qw improvement
- Fig 2: utterance encoder, context representation, softmax
- Table 6, add F1
- Experiment 2 -> add L/NL condition
- Experiment 3 -> as well add laughters
* reviewer 3 
- say something about the split. How is it different from the
  "canonical split" (optional)
- CNN: "a simple and popular baseline for text classification or encoding texts"
