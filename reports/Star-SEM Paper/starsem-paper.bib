
@book{austinHowThingsWords2009,
  title = {How to Do Things with Words: The {{William James}} Lectures Delivered at {{Harvard University}} in 1955},
  shorttitle = {How to Do Things with Words},
  author = {Austin, John L. and Urmson, James O.},
  year = {2009},
  edition = {2. ed., [repr.]},
  publisher = {{Harvard Univ. Press}},
  address = {{Cambridge, Mass}},
  annotation = {OCLC: 935786421},
  file = {/Users/xnobwi/.zotero/storage/2MCXA76R/Austin - HOW TO DO THINGS WITH WORDS.pdf},
  isbn = {978-0-674-41152-4},
  language = {eng}
}

@article{baoPLATOPretrainedDialogue2019,
  title = {{{PLATO}}: {{Pre}}-Trained {{Dialogue Generation Model}} with {{Discrete Latent Variable}}},
  shorttitle = {{{PLATO}}},
  author = {Bao, Siqi and He, Huang and Wang, Fan and Wu, Hua},
  year = {2019},
  month = oct,
  abstract = {Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle with the natural born one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.},
  archivePrefix = {arXiv},
  eprint = {1910.07931},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/XCZNIHJ7/Bao et al. - 2019 - PLATO Pre-trained Dialogue Generation Model with .pdf},
  journal = {arXiv:1910.07931 [cs]},
  keywords = {Computer Science - Computation and Language,to-read},
  primaryClass = {cs}
}

@inproceedings{botheContextbasedApproachDialogue2018,
  title = {A {{Context}}-Based {{Approach}} for {{Dialogue Act Recognition}} Using {{Simple Recurrent Neural Networks}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Bothe, Chandrakant and Weber, Cornelius and Magg, Sven and Wermter, Stefan},
  year = {2018},
  month = may,
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Miyazaki, Japan}},
  file = {/Users/xnobwi/.zotero/storage/LDUY9W8Y/Bothe et al. - 2018 - A Context-based Approach for Dialogue Act Recognit.pdf}
}

@article{cerisaraEffectsUsingWord2vec2017,
  title = {On the Effects of Using Word2vec Representations in Neural Networks for Dialogue Act Recognition},
  author = {Cerisara, Christophe and Kr{\'a}l, Pavel and Lenc, Ladislav},
  year = {2017},
  volume = {47},
  pages = {175--193},
  issn = {08852308},
  doi = {10.1016/j.csl.2017.07.009},
  file = {/Users/xnobwi/.zotero/storage/4WXBE8SF/1-s2.0-S0885230816300456-main.pdf},
  journal = {Computer Speech \& Language},
  language = {en}
}

@article{chenDialogueActRecognition2017,
  title = {Dialogue {{Act Recognition}} via {{CRF}}-{{Attentive Structured Network}}},
  author = {Chen, Zheqian and Yang, Rongqin and Zhao, Zhou and Cai, Deng and He, Xiaofei},
  year = {2017},
  month = nov,
  abstract = {Dialogue Act Recognition (DAR) is a challenging problem in dialogue interpretation, which aims to attach semantic labels to utterances and characterize the speaker's intention. Currently, many existing approaches formulate the DAR problem ranging from multi-classification to structured prediction, which suffer from handcrafted feature extensions and attentive contextual structural dependencies. In this paper, we consider the problem of DAR from the viewpoint of extending richer Conditional Random Field (CRF) structural dependencies without abandoning end-to-end training. We incorporate hierarchical semantic inference with memory mechanism on the utterance modeling. We then extend structured attention network to the linear-chain conditional random field layer which takes into account both contextual utterances and corresponding dialogue acts. The extensive experiments on two major benchmark datasets Switchboard Dialogue Act (SWDA) and Meeting Recorder Dialogue Act (MRDA) datasets show that our method achieves better performance than other state-of-the-art solutions to the problem. It is a remarkable fact that our method is nearly close to the human annotator's performance on SWDA within 2\% gap.},
  archivePrefix = {arXiv},
  eprint = {1711.05568},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/Q4MKJ7GY/Chen et al. - 2017 - Dialogue Act Recognition via CRF-Attentive Structu.pdf},
  journal = {arXiv:1711.05568 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@inproceedings{chenSemanticallyConditionedDialog2019a,
  title = {Semantically {{Conditioned Dialog Response Generation}} via {{Hierarchical Disentangled Self}}-{{Attention}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Chen, Wenhu and Chen, Jianshu and Qin, Pengda and Yan, Xifeng and Wang, William Yang},
  year = {2019},
  month = jul,
  pages = {3696--3709},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1360},
  abstract = {Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.},
  file = {/Users/xnobwi/.zotero/storage/62JRNAFP/Chen et al. - 2019 - Semantically Conditioned Dialog Response Generatio.pdf}
}

@inproceedings{coreCodingDialogsDAMSL1997,
  title = {Coding {{Dialogs}} with the {{DAMSL Annotation Scheme}}},
  booktitle = {Working {{Notes}} of the {{AAAI Fall Symposium}} on {{Communicative Action}} in {{Humans}} and {{Machines}}},
  author = {Core, Mark G and Allen, James F},
  year = {1997},
  pages = {28--35},
  address = {{Boston, MA}},
  abstract = {This paper describes the DAMSL annotation scheme for communicative acts in dialog. The scheme has three layers: Forward Communicative Functions, Backward Communicative Functions, and Utterance Features. Each layer allows multiple communicative functions of an utterance to be labeled. The Forward Communicative Functions consist of a taxonomy in a similar style as the actions of traditional speech act theory. The Backward Communicative Functions indicate how the current utterance relates to the previous dialog, such as accepting a proposal, con rming understanding, or answering a question. The Utterance Features include information about an utterance's form and content, such as whether an utterance concerns the communication process itself or deals with the subject at hand. The kappa inter-annotator reliability scores for the rst test of DAMSL with human annotators show promise, but are on average 0.15 lower than the accepted kappa scores for such annotations. However, the slight revisions to DAMSL discussed here should increase accuracy on the next set of tests and produce a reliable, exible, and comprehensive utterance annotation scheme.},
  file = {/Users/xnobwi/.zotero/storage/BKUWT57X/Core and Allen - Coding Dialogs with the DAMSL Annotation Scheme.pdf},
  language = {en}
}

@article{devlinBERTPretrainingDeep2018,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  month = oct,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-theart models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/CD98FF2E/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@misc{GuidelinesDialogueAct2005,
  title = {Guidelines for {{Dialogue Act}} and {{Addressee Annotation Version}} 1.0},
  year = {2005},
  month = oct,
  file = {/Users/xnobwi/.zotero/storage/7QSQ6H7G/dialogue_acts_manual_1.0.pdf}
}

@article{Johnson2017,
  title = {Google's {{Multilingual Neural Machine Translation System}}: {{Enabling Zero}}-{{Shot Translation}}},
  shorttitle = {Google's {{Multilingual Neural Machine Translation System}}},
  author = {Johnson, Melvin and Schuster, Mike and Le, Quoc V. and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernanda and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
  year = {2017},
  month = dec,
  volume = {5},
  pages = {339--351},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00065},
  abstract = {We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT'14 benchmarks, a single multilingual model achieves comparable performance for English\textrightarrow French and surpasses state-of-theart results for English\textrightarrow German. Similarly, a single multilingual model surpasses stateof-the-art results for French\textrightarrow English and German\textrightarrow English on WMT'14 and WMT'15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zeroshot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.},
  file = {/Users/xnobwi/.zotero/storage/A5JNL8MW/Johnson et al. - 2017 - Googleâ€™s Multilingual Neural Machine Translation S.pdf},
  journal = {Transactions of the Association for Computational Linguistics},
  language = {en}
}

@article{jurafskyLexicalProsodicSyntactic,
  title = {Lexical, {{Prosodic}}, and {{Syntactic Cues}} for {{Dialog Acts}}},
  author = {Jurafsky, Daniel and Shriberg, Elizabeth and Fox, Barbara and Curl, Traci},
  pages = {7},
  file = {/Users/xnobwi/.zotero/storage/P2CXTYCL/Jurafsky et al. - Lexical, Prosodic, and Syntactic Cues for Dialog A.pdf},
  language = {en}
}

@misc{jurafskySwitchboardSWBDDAMSLShallowDiscourseFunction1997a,
  title = {Switchboard {{SWBD}}-{{DAMSL Shallow}}-{{Discourse}}-{{Function Annotation Coders Manual}}},
  author = {Jurafsky, Daniel and Shriberg, Liz and Biasca, Debra},
  year = {1997}
}

@inproceedings{kalchbrennerRecurrentConvolutionalNeural2013,
  title = {Recurrent {{Convolutional Neural Networks}} for {{Discourse Compositionality}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Continuous Vector Space Models}} and Their {{Compositionality}}},
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  year = {2013},
  pages = {119--126},
  abstract = {The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. The discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers. Without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classification experiment.},
  file = {/Users/xnobwi/.zotero/storage/SRUYIRW5/Kalchbrenner and Blunsom - Recurrent Convolutional Neural Networks for Discou.pdf},
  language = {en}
}

@inproceedings{khanpourDialogueActClassification2016,
  title = {Dialogue {{Act Classification}} in {{Domain}}-{{Independent Conversations Using}} a {{Deep Recurrent Neural Network}}},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Khanpour, Hamed and Guntakandla, Nishitha and Nielsen, Rodney},
  year = {2016},
  month = dec,
  pages = {2012--2021},
  address = {{Osaka, Japan}},
  abstract = {In this study, we applied a deep LSTM structure to classify dialogue acts (DAs) in open-domain conversations. We found that the word embeddings parameters, dropout regularization, decay rate and number of layers are the parameters that have the largest effect on the final system accuracy. Using the findings of these experiments, we trained a deep LSTM network that outperforms the state-of-the-art on the Switchboard corpus by 3.11\%, and MRDA by 2.2\%.},
  file = {/Users/xnobwi/.zotero/storage/79IVLLM8/Khanpour et al. - Dialogue Act Classification in Domain-Independent .pdf},
  language = {en}
}

@article{kimConvolutionalNeuralNetworks2014,
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  author = {Kim, Yoon},
  year = {2014},
  month = sep,
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  archivePrefix = {arXiv},
  eprint = {1408.5882},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/YITJI45Q/Kim - 2014 - Convolutional Neural Networks for Sentence Classif.pdf},
  journal = {arXiv:1408.5882 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{Lison2016,
  title = {{{OpenSubtitles2016}}: {{Extracting Large Parallel Corpora}} from {{Movie}} and {{TV Subtitles}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Language Resources}} and {{Evaluation}}},
  author = {Lison, Pierre and Tiedemann, Jorg},
  year = {2016},
  pages = {7},
  abstract = {We present a new major release of the OpenSubtitles collection of parallel corpora. The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs.},
  file = {/Users/xnobwi/.zotero/storage/DQ3AE7MC/Lison and Tiedemann - OpenSubtitles2016 Extracting Large Parallel Corpo.pdf},
  language = {en}
}

@article{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archivePrefix = {arXiv},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/EKNK5GA9/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf},
  journal = {arXiv:1907.11692 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@inproceedings{mehriPretrainingMethodsDialog2019,
  title = {Pretraining {{Methods}} for {{Dialog Context Representation Learning}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Mehri, Shikib and Razumovskaia, Evgeniia and Zhao, Tiancheng and Eskenazi, Maxine},
  year = {2019},
  pages = {3836--3845},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1373},
  abstract = {This paper examines various unsupervised pretraining objectives for learning dialog context representations. Two novel methods of pretraining dialog context encoders are proposed, and a total of four methods are examined. Each pretraining objective is fine-tuned and evaluated on a set of downstream dialog tasks using the MultiWoz dataset and strong performance improvement is observed. Further evaluation shows that our pretraining objectives result in not only better performance, but also better convergence, models that are less data hungry and have better domain generalizability.},
  file = {/Users/xnobwi/.zotero/storage/YGZWYMI5/Mehri et al. - 2019 - Pretraining Methods for Dialog Context Representat.pdf},
  language = {en}
}

@article{mehriStructuredFusionNetworks2019,
  title = {Structured {{Fusion Networks}} for {{Dialog}}},
  author = {Mehri, Shikib and Srinivasan, Tejas and Eskenazi, Maxine},
  year = {2019},
  month = jul,
  abstract = {Neural dialog models have exhibited strong performance, however their end-to-end nature lacks a representation of the explicit structure of dialog. This results in a loss of generalizability, controllability and a datahungry nature. Conversely, more traditional dialog systems do have strong models of explicit structure. This paper introduces several approaches for explicitly incorporating structure into neural models of dialog. Structured Fusion Networks first learn neural dialog modules corresponding to the structured components of traditional dialog systems and then incorporate these modules in a higher-level generative model. Structured Fusion Networks obtain strong results on the MultiWOZ dataset, both with and without reinforcement learning. Structured Fusion Networks are shown to have several valuable properties, including better domain generalizability, improved performance in reduced data scenarios and robustness to divergence during reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1907.10016},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/RNT7SBKH/Mehri et al. - 2019 - Structured Fusion Networks for Dialog.pdf},
  journal = {arXiv:1907.10016 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,to-read},
  language = {en},
  primaryClass = {cs}
}

@article{Mikolov2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  year = {2013},
  pages = {9},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
  file = {/Users/xnobwi/.zotero/storage/FPRM9TKD/Mikolov et al. - Distributed Representations of Words and Phrases a.pdf},
  journal = {NIPS Proceedings},
  language = {en}
}

@inproceedings{ortegaNeuralbasedContextRepresentation2017,
  title = {Neural-Based {{Context Representation Learning}} for {{Dialog Act Classification}}},
  booktitle = {Proceedings of the 18th {{Annual SIGdial Meeting}} on {{Discourse}} and {{Dialogue}}},
  author = {Ortega, Daniel and Vu, Ngoc Thang},
  year = {2017},
  pages = {247--252},
  publisher = {{Association for Computational Linguistics}},
  address = {{Saarbr\"ucken, Germany}},
  doi = {10.18653/v1/W17-5530},
  abstract = {We explore context representation learning methods in neural-based models for dialog act classification. We propose and compare extensively different methods which combine recurrent neural network architectures and attention mechanisms (AMs) at different context levels. Our experimental results on two benchmark datasets show consistent improvements compared to the models without contextual information and reveal that the most suitable AM in the architecture depends on the nature of the dataset.},
  file = {/Users/xnobwi/.zotero/storage/7YWQZXWG/Ortega and Vu - 2017 - Neural-based Context Representation Learning for D.pdf;/Users/xnobwi/.zotero/storage/F87RLCJT/Ortega and Vu - 2017 - Neural-based Context Representation Learning for D.pdf},
  language = {en}
}

@inproceedings{penningtonGloveGlobalVectors2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1162},
  file = {/Users/xnobwi/.zotero/storage/L5YPWP53/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf}
}

@article{petersTuneNotTune2019,
  title = {To {{Tune}} or {{Not}} to {{Tune}}? {{Adapting Pretrained Representations}} to {{Diverse Tasks}}},
  shorttitle = {To {{Tune}} or {{Not}} to {{Tune}}?},
  author = {Peters, Matthew and Ruder, Sebastian and Smith, Noah A.},
  year = {2019},
  month = mar,
  abstract = {While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.},
  archivePrefix = {arXiv},
  eprint = {1903.05987},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/GL2C9FD2/Peters et al. - 2019 - To Tune or Not to Tune Adapting Pretrained Repres.pdf},
  journal = {arXiv:1903.05987 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,to-read},
  primaryClass = {cs}
}

@inproceedings{pragstVectorRepresentationUtterances2018,
  title = {On the {{Vector Representation}} of {{Utterances}} in {{Dialogue Context}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}-2018)},
  author = {Pragst, Louisa and Rach, Niklas and Minker, Wolfgang and Ultes, Stefan},
  year = {2018},
  publisher = {{European Language Resource Association}},
  file = {/Users/xnobwi/.zotero/storage/3DEGXJPE/Pragst et al. - 2018 - On the Vector Representation of Utterances in Dial.pdf},
  keywords = {suggestion}
}

@article{shenNeuralAttentionModels2016,
  title = {Neural {{Attention Models}} for {{Sequence Classification}}: {{Analysis}} and {{Application}} to {{Key Term Extraction}} and {{Dialogue Act Detection}}},
  shorttitle = {Neural {{Attention Models}} for {{Sequence Classification}}},
  author = {Shen, Sheng-syun and Lee, Hung-yi},
  year = {2016},
  month = mar,
  abstract = {Recurrent neural network architectures combining with attention mechanism, or neural attention model, have shown promising performance recently for the tasks including speech recognition, image caption generation, visual question answering and machine translation. In this paper, neural attention model is applied on two sequence classification tasks, dialogue act detection and key term extraction. In the sequence labeling tasks, the model input is a sequence, and the output is the label of the input sequence. The major difficulty of sequence labeling is that when the input sequence is long, it can include many noisy or irrelevant part. If the information in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance. The attention mechanism is helpful for sequence classification task because it is capable of highlighting important part among the entire sequence for the classification task. The experimental results show that with the attention mechanism, discernible improvements were achieved in the sequence labeling task considered here. The roles of the attention mechanism in the tasks are further analyzed and visualized in this paper.},
  archivePrefix = {arXiv},
  eprint = {1604.00077},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/DGLRKFZP/Shen and Lee - 2016 - Neural Attention Models for Sequence Classificatio.pdf},
  journal = {arXiv:1604.00077 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@inproceedings{sordoniNeuralNetworkApproach2015,
  title = {A {{Neural Network Approach}} to {{Context}}-{{Sensitive Generation}} of {{Conversational Responses}}},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, Bill},
  year = {2015},
  pages = {196--205},
  publisher = {{Association for Computational Linguistics}},
  address = {{Denver, Colorado}},
  doi = {10.3115/v1/N15-1020},
  abstract = {We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.},
  file = {/Users/xnobwi/.zotero/storage/6QR84V9B/Sordoni et al. - 2015 - A Neural Network Approach to Context-Sensitive Gen.pdf},
  language = {en}
}

@article{stolckeDialogueActModeling2000,
  ids = {stolckeDialogueActModeling2000a},
  title = {Dialogue {{Act Modeling}} for {{Automatic Tagging}} and {{Recognition}} of {{Conversational Speech}}},
  author = {Stolcke, Andreas and Ries, Klaus and Coccaro, Noah and Shriberg, Elizabeth and Bates, Rebecca and Jurafsky, Daniel and Taylor, Paul and Martin, Rachel and {Ess-Dykema}, Carol Van and Meteer, Marie},
  year = {2000},
  month = sep,
  volume = {26},
  pages = {339--373},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/089120100561737},
  file = {/Users/xnobwi/.zotero/storage/E2RM82HV/Stolcke et al. - 2000 - Dialogue Act Modeling for Automatic Tagging and Re.pdf},
  journal = {Computational Linguistics},
  language = {en},
  number = {3}
}

@inproceedings{Subramanian2019,
  title = {Target {{Based Speech Act Classification}} in {{Political Campaign Text}}},
  booktitle = {Proceedings of the {{Eighth Joint Conference}} on {{Lexical}} and {{Computational Semantics}} (*{{SEM}} 2019)},
  author = {Subramanian, Shivashankar and Cohn, Trevor and Baldwin, Timothy},
  year = {2019},
  pages = {273--282},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/S19-1030},
  abstract = {We study pragmatics in political campaign text, through analysis of speech acts and the target of each utterance. We propose a new annotation schema incorporating domainspecific speech acts, such as commissiveaction, and present a novel annotated corpus of media releases and speech transcripts from the 2016 Australian election cycle. We show how speech acts and target referents can be modeled as sequential classification, and evaluate several techniques, exploiting contextualized word representations, semi-supervised learning, task dependencies and speaker meta-data.},
  file = {/Users/xnobwi/.zotero/storage/CY5LCPWI/Subramanian et al. - 2019 - Target Based Speech Act Classification in Politica.pdf},
  language = {en}
}

@article{sunHowFineTuneBERT2019,
  title = {How to {{Fine}}-{{Tune BERT}} for {{Text Classification}}?},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  year = {2019},
  month = aug,
  abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
  archivePrefix = {arXiv},
  eprint = {1905.05583},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/42ZLQ7BG/Sun et al. - 2019 - How to Fine-Tune BERT for Text Classification.pdf},
  journal = {arXiv:1905.05583 [cs]},
  keywords = {Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{tranHierarchicalNeuralModel2017,
  title = {A {{Hierarchical Neural Model}} for {{Learning Sequences}} of {{Dialogue Acts}}},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the           {{Association}} for {{Computational Linguistics}}: {{Volume}} 1, {{Long Papers}}},
  author = {Tran, Quan Hung and Zukerman, Ingrid and Haffari, Gholamreza},
  year = {2017},
  pages = {428--437},
  publisher = {{Association for Computational Linguistics}},
  address = {{Valencia, Spain}},
  doi = {10.18653/v1/E17-1041},
  abstract = {We propose a novel hierarchical Recurrent Neural Network (RNN) for learning sequences of Dialogue Acts (DAs). The input in this task is a sequence of utterances (i.e., conversational contributions) comprising a sequence of tokens, and the output is a sequence of DA labels (one label per utterance). Our model leverages the hierarchical nature of dialogue data by using two nested RNNs that capture long-range dependencies at the dialogue level and the utterance level. This model is combined with an attention mechanism that focuses on salient tokens in utterances. Our experimental results show that our model outperforms strong baselines on two popular datasets, Switchboard and MapTask; and our detailed empirical analysis highlights the impact of each aspect of our model.},
  file = {/Users/xnobwi/.zotero/storage/FDT4SJK7/Tran et al. - 2017 - A Hierarchical Neural Model for Learning Sequences.pdf},
  language = {en}
}

@inproceedings{tranPreservingDistributionalInformation2017,
  title = {Preserving {{Distributional Information}} in {{Dialogue Act Classification}}},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Tran, Quan Hung and Zukerman, Ingrid and Haffari, Gholamreza},
  year = {2017},
  pages = {2151--2156},
  publisher = {{Association for Computational Linguistics}},
  address = {{Copenhagen, Denmark}},
  doi = {10.18653/v1/D17-1229},
  abstract = {This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively simple, it outperforms more complex neural models, achieving state-of-the-art results on the MapTask and Switchboard corpora.},
  file = {/Users/xnobwi/.zotero/storage/LDF2M2KJ/Tran et al. - 2017 - Preserving Distributional Information in Dialogue .pdf},
  language = {en}
}

@inproceedings{vigComparisonTransferLearningApproaches2019,
  title = {Comparison of {{Transfer}}-{{Learning Approaches}} for {{Response Selection}} in {{Multi}}-{{Turn Conversations}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Dialog System Technology Challenges}}},
  author = {Vig, Jesse and Ramea, Kalai},
  year = {2019},
  month = jan,
  pages = {7},
  address = {{Honolulu, Hawaii}},
  abstract = {This paper compares three transfer-learning approaches to response selection in dialogs, as part of the Dialog System Technology Challenge 7 (DSTC7) Track 1. In the first approach, Multi-Turn ESIM+ELMo (MT-EE), we incorporate pre-trained contextual embeddings into a sentence-pair model that was originally designed for natural language inference. In the second approach, we fine-tune the Generative Pre-trained Transformer (OpenAI GPT) model. In the third approach, we fine-tune the Bidirectional Encoder Representations from Transformers (BERT) model. Our results show that BERT performed best, followed by the GPT model and then the MTEE model. We also discuss the relative advantages and disadvantages of each approach. The submitted result for Track 1 (MT-EE) placed second and fifth overall for the Advising and Ubuntu datasets respectively.},
  file = {/Users/xnobwi/.zotero/storage/93DAJXW4/Vig and Ramea - Comparison of Transfer-Learning Approaches for Res.pdf},
  language = {en}
}

@article{wolfHuggingFaceTransformersStateoftheart2019,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State}}-of-the-Art {{Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Brew, Jamie},
  year = {2019},
  month = oct,
  abstract = {Recent advances in modern Natural Language Processing (NLP) research have been dominated by the combination of Transfer Learning methods with large-scale language models, in particular based on the Transformer architecture. With them came a paradigm shift in NLP with the starting point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider NLP community. In this paper, we present HuggingFace's Transformers library, a library for state-of-the-art NLP, making these developments available to the community by gathering state-of-the-art general-purpose pretrained models under a unified API together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream NLP tasks. HuggingFace's Transformers library features carefully crafted model implementations and high-performance pretrained weights for two main deep learning frameworks, PyTorch and TensorFlow, while supporting all the necessary tools to analyze, evaluate and use these models in downstream tasks such as text/token classification, questions answering and language generation among others. The library has gained significant organic traction and adoption among both the researcher and practitioner communities. We are committed at HuggingFace to pursue the efforts to develop this toolkit with the ambition of creating the standard library for building NLP systems.},
  archivePrefix = {arXiv},
  eprint = {1910.03771},
  eprinttype = {arxiv},
  file = {/Users/xnobwi/.zotero/storage/FH8M8CJC/Wolf et al. - 2019 - HuggingFace's Transformers State-of-the-art Natur.pdf},
  journal = {arXiv:1910.03771 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}
