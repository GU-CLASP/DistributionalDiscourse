%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{booktabs}
\usepackage{tabularx}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

\usepackage{graphicx}
\usepackage{placeins}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Large-scale text pre-training helps with dialogue act recognition, but not without fine-tuning\\\textit{Appendix}}


\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\date{}

% We should make the comparison with other tasks where BERT representations work well AS IS (i.e., without finetuning) vs dialogue where 
% the pretraining is helpful, but not without finetuning
% Missing reference(s) from *SEM reviews
% Usefulness of in-domain pre-training for spoken dialogue
% Future work: dialogue act recognition is not enough to fully evaluate BERT for dialogue - other turn change prediction, intent classification, utterance segmentation and disfluency detection
%  

\begin{document}
\maketitle

%\appendix

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\linewidth]{img/swda-brf.pdf}
  \includegraphics[width=\linewidth]{img/ami-brf.pdf}
  \caption{F1 scores by dialogue act for BERT with standard pre-training and DAR fine-tuning (\texttt{BERT-FT}) vs.~the same model without pre-training (\texttt{BERT-RI}) and without fine-tuning (\texttt{BERT-FZ}). Dialogue acts are ordered with the most common on the left.}
    \label{fig:f1-by-da}
  \end{figure*}

%\FloatBarrier
%\bibliographystyle{acl_natbib}
%\bibliography{acl2020}

\end{document}
