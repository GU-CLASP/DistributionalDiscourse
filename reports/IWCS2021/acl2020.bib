
@inproceedings{coreCodingDialogsDAMSL1997,
  address = {{Boston, MA}},
  title = {Coding {{Dialogs}} with the {{DAMSL Annotation Scheme}}},
  abstract = {This paper describes the DAMSL annotation scheme for communicative acts in dialog. The scheme has three layers: Forward Communicative Functions, Backward Communicative Functions, and Utterance Features. Each layer allows multiple communicative functions of an utterance to be labeled. The Forward Communicative Functions consist of a taxonomy in a similar style as the actions of traditional speech act theory. The Backward Communicative Functions indicate how the current utterance relates to the previous dialog, such as accepting a proposal, con rming understanding, or answering a question. The Utterance Features include information about an utterance's form and content, such as whether an utterance concerns the communication process itself or deals with the subject at hand. The kappa inter-annotator reliability scores for the rst test of DAMSL with human annotators show promise, but are on average 0.15 lower than the accepted kappa scores for such annotations. However, the slight revisions to DAMSL discussed here should increase accuracy on the next set of tests and produce a reliable, exible, and comprehensive utterance annotation scheme.},
  language = {en},
  booktitle = {Working {{Notes}} of the {{AAAI Fall Symposium}} on {{Communicative Action}} in {{Humans}} and {{Machines}}},
  author = {Core, Mark G and Allen, James F},
  year = {1997},
  pages = {28-35},
}

@inproceedings{pragstVectorRepresentationUtterances2018,
  title = {On the {{Vector Representation}} of {{Utterances}} in {{Dialogue Context}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}}-2018)},
  publisher = {{European Language Resource Association}},
  author = {Pragst, Louisa and Rach, Niklas and Minker, Wolfgang and Ultes, Stefan},
  year = {2018},
  keywords = {suggestion},
}

@inproceedings{devlinBERTPretrainingDeep2018,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1423},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
}

@article{cerisaraEffectsUsingWord2vec2017,
  title = {On the Effects of Using Word2vec Representations in Neural Networks for Dialogue Act Recognition},
  volume = {47},
  issn = {08852308},
  language = {en},
  journal = {Computer Speech \& Language},
  doi = {10.1016/j.csl.2017.07.009},
  author = {Cerisara, Christophe and Kr{\'a}l, Pavel and Lenc, Ladislav},
  year = {2017},
  pages = {175-193},
}

@book{austinHowThingsWords2009,
  address = {{Cambridge, Mass}},
  edition = {2. ed., [repr.]},
  title = {How to Do Things with Words: The {{William James}} Lectures Delivered at {{Harvard University}} in 1955},
  isbn = {978-0-674-41152-4},
  shorttitle = {How to Do Things with Words},
  language = {eng},
  publisher = {{Harvard Univ. Press}},
  author = {Austin, John L. and Urmson, James O.},
  year = {2009},
  note = {OCLC: 935786421}
}

@inproceedings{kalchbrennerRecurrentConvolutionalNeural2013,
  title = {Recurrent {{Convolutional Neural Networks}} for {{Discourse Compositionality}}},
  abstract = {The compositionality of meaning extends beyond the single sentence. Just as words combine to form the meaning of sentences, so do sentences combine to form the meaning of paragraphs, dialogues and general discourse. We introduce both a sentence model and a discourse model corresponding to the two levels of compositionality. The sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network. The discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker. The discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers. Without feature engineering or pretraining and with simple greedy decoding, the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classification experiment.},
  language = {en},
  booktitle = {Proceedings of the {{Workshop}} on {{Continuous Vector Space Models}} and Their {{Compositionality}}},
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  year = {2013},
  pages = {119-126},
}

@article{stolckeDialogueActModeling2000,
  title = {Dialogue {{Act Modeling}} for {{Automatic Tagging}} and {{Recognition}} of {{Conversational Speech}}},
  volume = {26},
  issn = {0891-2017, 1530-9312},
  language = {en},
  number = {3},
  journal = {Computational Linguistics},
  doi = {10.1162/089120100561737},
  author = {Stolcke, Andreas and Ries, Klaus and Coccaro, Noah and Shriberg, Elizabeth and Bates, Rebecca and Jurafsky, Daniel and Taylor, Paul and Martin, Rachel and {Ess-Dykema}, Carol Van and Meteer, Marie},
  month = sep,
  year = {2000},
  pages = {339-373},
  ids = {stolckeDialogueActModeling2000a}
}

@inproceedings{sordoniNeuralNetworkApproach2015,
  address = {{Denver, Colorado}},
  title = {A {{Neural Network Approach}} to {{Context}}-{{Sensitive Generation}} of {{Conversational Responses}}},
  abstract = {We present a novel response generation system that can be trained end to end on large quantities of unstructured Twitter conversations. A neural network architecture is used to address sparsity issues that arise when integrating contextual information into classic statistical models, allowing the system to take into account previous dialog utterances. Our dynamic-context generative models show consistent gains over both context-sensitive and non-context-sensitive Machine Translation and Information Retrieval baselines.},
  language = {en},
  booktitle = {Proceedings of the 2015 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/v1/N15-1020},
  author = {Sordoni, Alessandro and Galley, Michel and Auli, Michael and Brockett, Chris and Ji, Yangfeng and Mitchell, Margaret and Nie, Jian-Yun and Gao, Jianfeng and Dolan, Bill},
  year = {2015},
  pages = {196-205},
}

@inproceedings{khanpourDialogueActClassification2016,
  address = {{Osaka, Japan}},
  title = {Dialogue {{Act Classification}} in {{Domain}}-{{Independent Conversations Using}} a {{Deep Recurrent Neural Network}}},
  abstract = {In this study, we applied a deep LSTM structure to classify dialogue acts (DAs) in open-domain conversations. We found that the word embeddings parameters, dropout regularization, decay rate and number of layers are the parameters that have the largest effect on the final system accuracy. Using the findings of these experiments, we trained a deep LSTM network that outperforms the state-of-the-art on the Switchboard corpus by 3.11\%, and MRDA by 2.2\%.},
  language = {en},
  booktitle = {Proceedings of {{COLING}} 2016, the 26th {{International Conference}} on {{Computational Linguistics}}: {{Technical Papers}}},
  author = {Khanpour, Hamed and Guntakandla, Nishitha and Nielsen, Rodney},
  month = dec,
  year = {2016},
  pages = {2012-2021},
}

@inproceedings{tranPreservingDistributionalInformation2017,
  address = {{Copenhagen, Denmark}},
  title = {Preserving {{Distributional Information}} in {{Dialogue Act Classification}}},
  abstract = {This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively simple, it outperforms more complex neural models, achieving state-of-the-art results on the MapTask and Switchboard corpora.},
  language = {en},
  booktitle = {Proceedings of the 2017 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/D17-1229},
  author = {Tran, Quan Hung and Zukerman, Ingrid and Haffari, Gholamreza},
  year = {2017},
  pages = {2151-2156},
}

@article{mikolovDistributedRepresentationsWords2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
  language = {en},
  journal = {NIPS Proceedings},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  year = {2013},
  pages = {9},
}

@inproceedings{chenDialogueActRecognition2017,
  title = {Dialogue {{Act Recognition}} via {{CRF}}-{{Attentive Structured Network}}},
  booktitle = {The 41st {{International ACM SIGIR Conference}} on {{Research}} \& {{Development}} in {{Information Retrieval}}},
  author = {Chen, Zheqian and Yang, Rongqin and Zhao, Zhou and Cai, Deng and He, Xiaofei},
  year = {2018},
  month = jun,
  pages = {225--234},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3209978.3209997},
  abstract = {Dialogue Act Recognition (DAR) is a challenging problem in dialogue interpretation, which aims to associate semantic labels to utterances and characterize the speaker's intention. Currently, many existing approaches formulate the DAR problem ranging from multi-classification to structured prediction, which suffer from handcrafted feature extensions and attentive contextual dependencies. In this paper, we tackle the problem of DAR from the viewpoint of extending richer Conditional Random Field (CRF) structured dependencies without abandoning end-to-end training. We incorporate hierarchical semantic inference with memory mechanism on the utterance modeling at multiple levels. We then utilize the structured attention network on the linear-chain CRF to dynamically separate the utterances into cliques. The extensive experiments on two primary benchmark datasets Switchboard Dialogue Act (SWDA) and Meeting Recorder Dialogue Act (MRDA) datasets show that our method achieves better performance than other state-of-the-art solutions to the problem.},
  isbn = {978-1-4503-5657-2},
  keywords = {conditional random field,dialogue act recognition,structured attention network},
  series = {{{SIGIR}} '18}
}

@inproceedings{ortegaNeuralbasedContextRepresentation2017,
  address = {{Saarbr{\"u}cken, Germany}},
  title = {Neural-Based {{Context Representation Learning}} for {{Dialog Act Classification}}},
  abstract = {We explore context representation learning methods in neural-based models for dialog act classification. We propose and compare extensively different methods which combine recurrent neural network architectures and attention mechanisms (AMs) at different context levels. Our experimental results on two benchmark datasets show consistent improvements compared to the models without contextual information and reveal that the most suitable AM in the architecture depends on the nature of the dataset.},
  language = {en},
  booktitle = {Proceedings of the 18th {{Annual SIGdial Meeting}} on {{Discourse}} and {{Dialogue}}},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/W17-5530},
  author = {Ortega, Daniel and Vu, Ngoc Thang},
  year = {2017},
  pages = {247-252},
}

@article{shenNeuralAttentionModels2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1604.00077},
  primaryClass = {cs},
  title = {Neural {{Attention Models}} for {{Sequence Classification}}: {{Analysis}} and {{Application}} to {{Key Term Extraction}} and {{Dialogue Act Detection}}},
  shorttitle = {Neural {{Attention Models}} for {{Sequence Classification}}},
  abstract = {Recurrent neural network architectures combining with attention mechanism, or neural attention model, have shown promising performance recently for the tasks including speech recognition, image caption generation, visual question answering and machine translation. In this paper, neural attention model is applied on two sequence classification tasks, dialogue act detection and key term extraction. In the sequence labeling tasks, the model input is a sequence, and the output is the label of the input sequence. The major difficulty of sequence labeling is that when the input sequence is long, it can include many noisy or irrelevant part. If the information in the whole sequence is treated equally, the noisy or irrelevant part may degrade the classification performance. The attention mechanism is helpful for sequence classification task because it is capable of highlighting important part among the entire sequence for the classification task. The experimental results show that with the attention mechanism, discernible improvements were achieved in the sequence labeling task considered here. The roles of the attention mechanism in the tasks are further analyzed and visualized in this paper.},
  journal = {arXiv:1604.00077 [cs]},
  author = {Shen, Sheng-syun and Lee, Hung-yi},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Computation and Language},
}

@inproceedings{tranHierarchicalNeuralModel2017,
  address = {{Valencia, Spain}},
  title = {A {{Hierarchical Neural Model}} for {{Learning Sequences}} of {{Dialogue Acts}}},
  abstract = {We propose a novel hierarchical Recurrent Neural Network (RNN) for learning sequences of Dialogue Acts (DAs). The input in this task is a sequence of utterances (i.e., conversational contributions) comprising a sequence of tokens, and the output is a sequence of DA labels (one label per utterance). Our model leverages the hierarchical nature of dialogue data by using two nested RNNs that capture long-range dependencies at the dialogue level and the utterance level. This model is combined with an attention mechanism that focuses on salient tokens in utterances. Our experimental results show that our model outperforms strong baselines on two popular datasets, Switchboard and MapTask; and our detailed empirical analysis highlights the impact of each aspect of our model.},
  language = {en},
  booktitle = {Proceedings of the 15th {{Conference}} of the {{European Chapter}} of the           {{Association}} for {{Computational Linguistics}}: {{Volume}} 1, {{Long Papers}}},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/E17-1041},
  author = {Tran, Quan Hung and Zukerman, Ingrid and Haffari, Gholamreza},
  year = {2017},
  pages = {428-437},
}

@inproceedings{petersTuneNotTune2019,
  title = {To {{Tune}} or {{Not}} to {{Tune}}? {{Adapting Pretrained Representations}} to {{Diverse Tasks}}},
  shorttitle = {To {{Tune}} or {{Not}} to {{Tune}}?},
  booktitle = {Proceedings of the 4th {{Workshop}} on {{Representation Learning}} for {{NLP}} ({{RepL4NLP}}-2019)},
  author = {Peters, Matthew E. and Ruder, Sebastian and Smith, Noah A.},
  year = {2019},
  pages = {7--14},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4302},
  abstract = {While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly finetuning the pretrained model. Our empirical results across diverse NLP tasks with two stateof-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.},
  language = {en}
}


@incollection{searleTaxonomyIllocutionaryActs1979,
  address = {{Cambridge}},
  title = {A {{Taxonomy}} of {{Illocutionary Acts}}},
  isbn = {978-0-511-60921-3},
  language = {en},
  booktitle = {Expression and {{Meaning}}: {{Studies}} in the {{Theory}} of {{Speech Acts}}},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/CBO9780511609213},
  author = {Searle, John R.},
  year = {1979},
}

@article{jurafskyLexicalProsodicSyntactic,
  title = {Lexical, {{Prosodic}}, and {{Syntactic Cues}} for {{Dialog Acts}}},
  language = {en},
  author = {Jurafsky, Daniel and Shriberg, Elizabeth and Fox, Barbara and Curl, Traci},
  pages = {7},
}

@inproceedings{baoPLATOPretrainedDialogue2019,
  title = {{{PLATO}}: {{Pre}}-Trained {{Dialogue Generation Model}} with {{Discrete Latent Variable}}},
  shorttitle = {{{PLATO}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Bao, Siqi and He, Huang and Wang, Fan and Wu, Hua and Wang, Haifeng},
  year = {2020},
  month = jul,
  pages = {85--96},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.9},
  abstract = {Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.},
}


@article{mehriStructuredFusionNetworks2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1907.10016},
  primaryClass = {cs},
  title = {Structured {{Fusion Networks}} for {{Dialog}}},
  abstract = {Neural dialog models have exhibited strong performance, however their end-to-end nature lacks a representation of the explicit structure of dialog. This results in a loss of generalizability, controllability and a datahungry nature. Conversely, more traditional dialog systems do have strong models of explicit structure. This paper introduces several approaches for explicitly incorporating structure into neural models of dialog. Structured Fusion Networks first learn neural dialog modules corresponding to the structured components of traditional dialog systems and then incorporate these modules in a higher-level generative model. Structured Fusion Networks obtain strong results on the MultiWOZ dataset, both with and without reinforcement learning. Structured Fusion Networks are shown to have several valuable properties, including better domain generalizability, improved performance in reduced data scenarios and robustness to divergence during reinforcement learning.},
  language = {en},
  journal = {arXiv:1907.10016 [cs]},
  author = {Mehri, Shikib and Srinivasan, Tejas and Eskenazi, Maxine},
  month = jul,
  year = {2019},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Computation and Language,to-read},
}

@article{hancherClassificationCooperativeIllocutionary1979,
  title = {The {{Classification}} of {{Cooperative Illocutionary Acts}}},
  volume = {8},
  issn = {0047-4045},
  abstract = {The different taxonomies of illocutionary acts proposed by Austin, Searle, Vendler, Ohmann, and Fraser are compared in summary form, with Searle's taxonomy taken as a reference standard. All five of these taxonomies slight two kinds of illocutionary act: (1) illocutionary acts that combine commissive with directive illocutionary force (e.g., offering, inviting, challenging), and (2) illocutionary acts that require two participants (e.g., giving, selling, contracting). These and related speech acts are discussed in some detail, and Searle's classification is amended to take them into account.},
  number = {1},
  journal = {Language in Society},
  author = {Hancher, Michael},
  year = {1979},
  pages = {1-14},
}

@inproceedings{chenSemanticallyConditionedDialog2019a,
  address = {{Florence, Italy}},
  title = {Semantically {{Conditioned Dialog Response Generation}} via {{Hierarchical Disentangled Self}}-{{Attention}}},
  abstract = {Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/P19-1360},
  author = {Chen, Wenhu and Chen, Jianshu and Qin, Pengda and Yan, Xifeng and Wang, William Yang},
  month = jul,
  year = {2019},
  pages = {3696--3709},
}

@inproceedings{mehriPretrainingMethodsDialog2019,
  address = {{Florence, Italy}},
  title = {Pretraining {{Methods}} for {{Dialog Context Representation Learning}}},
  abstract = {This paper examines various unsupervised pretraining objectives for learning dialog context representations. Two novel methods of pretraining dialog context encoders are proposed, and a total of four methods are examined. Each pretraining objective is fine-tuned and evaluated on a set of downstream dialog tasks using the MultiWoz dataset and strong performance improvement is observed. Further evaluation shows that our pretraining objectives result in not only better performance, but also better convergence, models that are less data hungry and have better domain generalizability.},
  language = {en},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/P19-1373},
  author = {Mehri, Shikib and Razumovskaia, Evgeniia and Zhao, Tiancheng and Eskenazi, Maxine},
  year = {2019},
  pages = {3836-3845},
}

@inproceedings{vigComparisonTransferLearningApproaches2019,
  address = {{Honolulu, Hawaii}},
  title = {Comparison of {{Transfer}}-{{Learning Approaches}} for {{Response Selection}} in {{Multi}}-{{Turn Conversations}}},
  abstract = {This paper compares three transfer-learning approaches to response selection in dialogs, as part of the Dialog System Technology Challenge 7 (DSTC7) Track 1. In the first approach, Multi-Turn ESIM+ELMo (MT-EE), we incorporate pre-trained contextual embeddings into a sentence-pair model that was originally designed for natural language inference. In the second approach, we fine-tune the Generative Pre-trained Transformer (OpenAI GPT) model. In the third approach, we fine-tune the Bidirectional Encoder Representations from Transformers (BERT) model. Our results show that BERT performed best, followed by the GPT model and then the MTEE model. We also discuss the relative advantages and disadvantages of each approach. The submitted result for Track 1 (MT-EE) placed second and fifth overall for the Advising and Ubuntu datasets respectively.},
  language = {en},
  booktitle = {Proceedings of the {{Workshop}} on {{Dialog System Technology Challenges}}},
  author = {Vig, Jesse and Ramea, Kalai},
  month = jan,
  year = {2019},
  pages = {7},
}

@article{kimConvolutionalNeuralNetworks2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1408.5882},
  primaryClass = {cs},
  title = {Convolutional {{Neural Networks}} for {{Sentence Classification}}},
  abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
  language = {en},
  journal = {arXiv:1408.5882 [cs]},
  author = {Kim, Yoon},
  month = sep,
  year = {2014},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Computation and Language},
}

@inproceedings{sunHowFineTuneBERT2019,
  title = {How to {{Fine}}-{{Tune BERT}} for {{Text Classification}}?},
  booktitle = {Chinese {{Computational Linguistics}}},
  author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  editor = {Sun, Maosong and Huang, Xuanjing and Ji, Heng and Liu, Zhiyuan and Liu, Yang},
  year = {2019},
  pages = {194--206},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-32381-3_16},
  abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
  isbn = {978-3-030-32381-3},
  keywords = {BERT,Text classification,Transfer learning},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@misc{jurafskySwitchboardSWBDDAMSLShallowDiscourseFunction1997a,
  title = {Switchboard {{SWBD}}-{{DAMSL Shallow}}-{{Discourse}}-{{Function Annotation Coders Manual}}},
  author = {Jurafsky, Daniel and Shriberg, Liz and Biasca, Debra},
  year = {1997}
}

@inproceedings{penningtonGloveGlobalVectors2014,
  address = {{Doha, Qatar}},
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/v1/D14-1162},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  month = oct,
  year = {2014},
  pages = {1532--1543},
}

@article{wolfHuggingFaceTransformersStateoftheart2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1910.03771},
  primaryClass = {cs},
  title = {{{HuggingFace}}'s {{Transformers}}: {{State}}-of-the-Art {{Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  abstract = {Recent advances in modern Natural Language Processing (NLP) research have been dominated by the combination of Transfer Learning methods with large-scale language models, in particular based on the Transformer architecture. With them came a paradigm shift in NLP with the starting point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider NLP community. In this paper, we present HuggingFace's Transformers library, a library for state-of-the-art NLP, making these developments available to the community by gathering state-of-the-art general-purpose pretrained models under a unified API together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream NLP tasks. HuggingFace's Transformers library features carefully crafted model implementations and high-performance pretrained weights for two main deep learning frameworks, PyTorch and TensorFlow, while supporting all the necessary tools to analyze, evaluate and use these models in downstream tasks such as text/token classification, questions answering and language generation among others. The library has gained significant organic traction and adoption among both the researcher and practitioner communities. We are committed at HuggingFace to pursue the efforts to develop this toolkit with the ambition of creating the standard library for building NLP systems.},
  journal = {arXiv:1910.03771 [cs]},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Brew, Jamie},
  month = oct,
  year = {2019},
  keywords = {Computer Science - Computation and Language},
}

@inproceedings{botheContextbasedApproachDialogue2018,
  address = {{Miyazaki, Japan}},
  title = {A {{Context}}-Based {{Approach}} for {{Dialogue Act Recognition}} Using {{Simple Recurrent Neural Networks}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  publisher = {{European Language Resources Association (ELRA)}},
  author = {Bothe, Chandrakant and Weber, Cornelius and Magg, Sven and Wermter, Stefan},
  month = may,
  year = {2018},
}

@misc{GuidelinesDialogueAct2005,
  title = {Guidelines for {{Dialogue Act}} and {{Addressee Annotation}}. Augmented Multiparty Interaction Project Document, v. 1.0},
  author = {{AMI Project}}, 
  day = 13,
  month = October,
  year = 2005,
}

@phdthesis{mazzocconi2019phd,
  title = {Laughter in interaction: semantics, pragmatics and child development},
  author = {Mazzocconi, Chiara},
  year = {2019},
  school = {Universit{\'e} de Paris},
}
                  
@inproceedings{tian2016we,
  title = {When do we laugh?},
  author = {Tian, Ye and Mazzocconi, Chiara and Ginzburg, Jonathan},
  booktitle = {Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  pages = {360--369},
  year = {2016},
}
                  
@inproceedings{ginzburg2015understanding,
  title = {Understanding Laughter},
  author = {Ginzburg, Jonathan and Breitholtz, Ellen and Cooper, Robin and Hough, Julian and Tian, Ye},
  booktitle = {Proceedings of the 20th Amsterdam Colloquium},
  year = {2015},
}

@article{bavelas2008gesturing,
  title={Gesturing on the telephone: Independent effects of dialogue and visibility},
  author={Bavelas, Janet and Gerwing, Jennifer and Sutton, Chantelle and Prevost, Danielle},
  journal={Journal of Memory and Language},
  volume={58},
  number={2},
  pages={495--520},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{tepperman2006yeah,
  title={" Yeah Right": Sarcasm Recognition for Spoken Dialogue Systems},
  author={Tepperman, Joseph and Traum, David and Narayanan, Shrikanth},
  booktitle={Ninth International Conference on Spoken Language Processing},
  year={2006}
}

@article{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arXiv},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  journal = {arXiv:1907.11692 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryclass = {cs}
}

@inproceedings{chakravarty2019dialog,
  title={Dialog Acts Classification for Question-Answer Corpora.},
  author={Chakravarty, Saurabh and Chava, Raja Venkata Satya Phanindra and Fox, Edward A},
  booktitle={ASAIL@ ICAIL},
  year={2019}
}

@article{ribeiro2019deep,
  title={Deep dialog act recognition using multiple token, segment, and context information representations},
  author={Ribeiro, Eug{\'e}nio and Ribeiro, Ricardo and de Matos, David Martins},
  journal={Journal of Artificial Intelligence Research},
  volume={66},
  pages={861--899},
  year={2019}
}

@article{yu2019midas,
  title={MIDAS: A dialog act annotation scheme for open domain human machine spoken conversations},
  author={Yu, Dian and Yu, Zhou},
  journal={arXiv preprint arXiv:1908.10023},
  year={2019}
}

@article{bothe2018conversational,
  title={Conversational Analysis Using Utterance-level Attention-based Bidirectional Recurrent Neural Networks},
  author={Bothe, Chandrakant and Magg, Sven and Weber, Cornelius and Wermter, Stefan},
  journal={Proc. Interspeech 2018},
  pages={996--1000},
  year={2018}
}

@inproceedings{zhao2018unified,
  title={A Unified Neural Architecture for Joint Dialog Act Segmentation and Recognition in Spoken Dialog System},
  author={Zhao, Tianyu and Kawahara, Tatsuya},
  booktitle={Proceedings of the 19th Annual SIGdial Meeting on Discourse and Dialogue},
  pages={201--208},
  year={2018}
}

@inproceedings{Lison2016,
  title = {{{OpenSubtitles2016}}: {{Extracting Large Parallel Corpora}} from {{Movie}} and {{TV Subtitles}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{Language Resources}} and {{Evaluation}}},
  author = {Lison, Pierre and Tiedemann, Jorg},
  year = {2016},
  pages = {7},
  abstract = {We present a new major release of the OpenSubtitles collection of parallel corpora. The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs.},
  language = {en}
}

@article{Carletta2007,
  title = {Unleashing the Killer Corpus: Experiences in Creating the Multi-Everything {{AMI Meeting Corpus}}},
  shorttitle = {Unleashing the Killer Corpus},
  author = {Carletta, Jean},
  year = {2007},
  month = nov,
  volume = {41},
  pages = {181--190},
  issn = {1574-020X, 1572-8412},
  doi = {10.1007/s10579-007-9040-x},
  abstract = {The AMI Meeting Corpus contains 100 hours of meetings captured using many synchronized recording devices, and is designed to support work in speech and video processing, language engineering, corpus linguistics, and organizational psychology. It has been transcribed orthographically, with annotated subsets for everything from named entities, dialogue acts, and summaries to simple gaze and head movement. In this written version of an LREC conference keynote address, I describe the data and how it was created. If this is ''killer'' data, that presupposes a platform that it will ''sell''; in this case, that is the NITE XML Toolkit, which allows a distributed set of users to create, store, browse, and search annotations for the same base data that are both time-aligned against signal and related to each other structurally.},
  journal = {Language Resources and Evaluation},
  language = {en},
  number = {2}
}

@inproceedings{Ravi2018,
  title = {Self-{{Governing Neural Networks}} for {{On}}-{{Device Short Text Classification}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Ravi, Sujith and Kozareva, Zornitsa},
  year = {2018},
  month = oct,
  pages = {887--893},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1105},
  abstract = {Deep neural networks reach state-of-the-art performance for wide range of natural language processing, computer vision and speech applications. Yet, one of the biggest challenges is running these complex networks on devices such as mobile phones or smart watches with tiny memory footprint and low computational capacity. We propose on-device Self-Governing Neural Networks (SGNNs), which learn compact projection vectors with local sensitive hashing. The key advantage of SGNNs over existing work is that they surmount the need for pre-trained word embeddings and complex networks with huge parameters. We conduct extensive evaluation on dialog act classification and show significant improvement over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations, while maintaining high accuracy.},
}

@inproceedings{Kozareva2019,
  ids = {Kozareva2019b},
  title = {{{ProSeqo}}: {{Projection Sequence Networks}} for {{On}}-{{Device Text Classification}}},
  shorttitle = {{{ProSeqo}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Kozareva, Zornitsa and Ravi, Sujith},
  year = {2019},
  month = nov,
  pages = {3894--3903},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1402},
  abstract = {We propose a novel on-device sequence model for text classification using recurrent projections. Our model ProSeqo uses dynamic recurrent projections without the need to store or look up any pre-trained embeddings. This results in fast and compact neural networks that can perform on-device inference for complex short and long text classification tasks. We conducted exhaustive evaluation on multiple text classification tasks. Results show that ProSeqo outperformed state-of-the-art neural and on-device approaches for short text classification tasks such as dialog act and intent prediction. To the best of our knowledge, ProSeqo is the first on-device long text classification neural model. It achieved comparable results to previous neural approaches for news article, answers and product categorization, while preserving small memory footprint and maintaining high accuracy.},
}

